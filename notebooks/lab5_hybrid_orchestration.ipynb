{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128d3c1e",
   "metadata": {},
   "source": [
    "# Lab 5: Hybrid Orchestration & Multi-Turn Conversation\n",
    "\n",
    "**Purpose:** Advanced multi-turn conversation management using the three-tier hybrid routing system (Local ‚Üí APIM ‚Üí Foundry Agents) with intelligent context preservation and seamless model switching.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab combines the hybrid routing system from Lab 4 with sophisticated conversation management:\n",
    "- **Three-tier routing**: Local, APIM Model Router, Foundry Agents\n",
    "- **Context preservation**: Chat history maintained across all routing targets\n",
    "- **Intelligent conversation**: Context-aware routing decisions\n",
    "- **Seamless switching**: Transparent model changes during conversation\n",
    "- **Enterprise features**: Session management and conversation analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04223772",
   "metadata": {},
   "source": [
    "## Step 5.1: Environment Setup and Import Hybrid Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory for module imports\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b49d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for module imports\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "modules_dir = os.path.join(parent_dir, 'modules')\n",
    "if modules_dir not in sys.path:\n",
    "    sys.path.append(modules_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "print(parent_dir)\n",
    "print(modules_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09980f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the hybrid router from Lab 4\n",
    "try:\n",
    "    from modules.hybrid_router import HybridFoundryAPIMRouter, HybridRouterConfig, create_hybrid_router_from_env\n",
    "    router_available = True\n",
    "    print(\"‚úÖ Hybrid router module imported successfully\")\n",
    "except ImportError as e:\n",
    "    router_available = False\n",
    "    print(f\"‚ùå Hybrid router not available: {e}\")\n",
    "    print(\"Please complete Lab 4 first or ensure modules are properly configured\")\n",
    "\n",
    "# Try to create the hybrid router from environment\n",
    "if router_available:\n",
    "    try:\n",
    "        hybrid_router = create_hybrid_router_from_env()\n",
    "        print(\"‚úÖ Hybrid router initialized from environment\")\n",
    "        \n",
    "        # Show available capabilities\n",
    "        capabilities = hybrid_router.get_system_capabilities()\n",
    "        print(f\"\\nüìä Available routing targets:\")\n",
    "        for target, available in capabilities['available_targets'].items():\n",
    "            status = \"‚úÖ\" if available else \"‚ùå\"\n",
    "            print(f\"   {target}: {status}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to create hybrid router: {e}\")\n",
    "        print(\"Will create fallback router\")\n",
    "        hybrid_router = None\n",
    "else:\n",
    "    hybrid_router = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d4e4d",
   "metadata": {},
   "source": [
    "## Step 5.2: Advanced Conversation Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridConversationManager:\n",
    "    \"\"\"Advanced conversation manager for three-tier hybrid routing system.\"\"\"\n",
    "    \n",
    "    def __init__(self, hybrid_router=None, max_history=15, session_id=None):\n",
    "        self.hybrid_router = hybrid_router\n",
    "        self.max_history = max_history\n",
    "        self.session_id = session_id or f\"session_{int(time.time())}\"\n",
    "        \n",
    "        # Conversation state\n",
    "        self.chat_history = []  # Full conversation history\n",
    "        self.conversation_stats = {\n",
    "            'total_exchanges': 0,\n",
    "            'local_responses': 0,\n",
    "            'apim_responses': 0,\n",
    "            'foundry_responses': 0,\n",
    "            'azure_responses': 0,\n",
    "            'mock_responses': 0,\n",
    "            'model_switches': 0,\n",
    "            'fallback_uses': 0,\n",
    "            'session_start': datetime.now(),\n",
    "            'last_source': None\n",
    "        }\n",
    "        \n",
    "        # Validate router availability\n",
    "        self.router_available = self._validate_router()\n",
    "        \n",
    "        print(f\"üé≠ Hybrid Conversation Manager initialized\")\n",
    "        print(f\"   Session ID: {self.session_id}\")\n",
    "        print(f\"   Max history: {self.max_history} exchanges\")\n",
    "        print(f\"   Router status: {'‚úÖ Available' if self.router_available else '‚ö†Ô∏è Mock mode'}\")\n",
    "    \n",
    "    def _validate_router(self):\n",
    "        \"\"\"Validate if the hybrid router is properly configured.\"\"\"\n",
    "        if not self.hybrid_router:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Test basic router functionality\n",
    "            capabilities = self.hybrid_router.get_system_capabilities()\n",
    "            available_targets = capabilities.get('available_targets', {})\n",
    "            \n",
    "            # Check if at least one target is available\n",
    "            has_working_target = any(available_targets.values())\n",
    "            \n",
    "            if not has_working_target:\n",
    "                print(\"‚ö†Ô∏è No routing targets available, will use mock responses\")\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Router validation failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _create_mock_response(self, user_message: str) -> str:\n",
    "        \"\"\"Create a mock response when routing fails.\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # Simple keyword-based mock routing\n",
    "        message_lower = user_message.lower()\n",
    "        \n",
    "        if any(word in message_lower for word in ['hello', 'hi', 'hey', 'greet']):\n",
    "            responses = [\n",
    "                \"[MOCK-LOCAL] Hello! I'm a mock local assistant.\",\n",
    "                \"[MOCK-LOCAL] Hi there! This is a simulated local response.\",\n",
    "                \"[MOCK-LOCAL] Hey! Mock local model responding.\"\n",
    "            ]\n",
    "            source = \"mock-local\"\n",
    "        elif any(word in message_lower for word in ['analyze', 'complex', 'detailed', 'comprehensive']):\n",
    "            responses = [\n",
    "                \"[MOCK-CLOUD] This is a simulated complex analysis from the cloud model. In a real scenario, this would provide detailed insights.\",\n",
    "                \"[MOCK-FOUNDRY] Mock Foundry Agent response: I would analyze this comprehensively using advanced AI capabilities.\",\n",
    "                \"[MOCK-APIM] Simulated APIM routing to enterprise model for complex analysis.\"\n",
    "            ]\n",
    "            source = \"mock-cloud\"\n",
    "        elif any(word in message_lower for word in ['calculate', 'math', '+', '-', '*', '/']):\n",
    "            responses = [\n",
    "                \"[MOCK-LOCAL] Simple calculation handled by mock local model.\",\n",
    "                \"[MOCK-LOCAL] Mock local response for mathematical query.\"\n",
    "            ]\n",
    "            source = \"mock-local\"\n",
    "        else:\n",
    "            responses = [\n",
    "                \"[MOCK-LOCAL] Mock response from local model for general query.\",\n",
    "                \"[MOCK-CLOUD] Mock response from cloud model for this query.\",\n",
    "                \"[MOCK-APIM] Simulated APIM routing response.\"\n",
    "            ]\n",
    "            source = \"mock-general\"\n",
    "        \n",
    "        response = random.choice(responses)\n",
    "        return f\"{response} [User asked: '{user_message[:50]}...']\"\n",
    "    \n",
    "    def _format_history_for_model(self, include_system=True) -> List[Dict]:\n",
    "        \"\"\"Format chat history for model consumption.\"\"\"\n",
    "        messages = []\n",
    "        \n",
    "        if include_system:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant in a hybrid routing system. Maintain conversation context and provide helpful responses.\"\n",
    "            })\n",
    "        \n",
    "        # Add recent conversation history\n",
    "        for exchange in self.chat_history[-self.max_history:]:\n",
    "            messages.append({\"role\": \"user\", \"content\": exchange['user_message']})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": exchange['response']})\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def _update_stats(self, source: str, was_fallback: bool = False):\n",
    "        \"\"\"Update conversation statistics.\"\"\"\n",
    "        self.conversation_stats['total_exchanges'] += 1\n",
    "        \n",
    "        # Track by source type\n",
    "        source_lower = source.lower()\n",
    "        if 'local' in source_lower:\n",
    "            self.conversation_stats['local_responses'] += 1\n",
    "        elif 'apim' in source_lower:\n",
    "            self.conversation_stats['apim_responses'] += 1\n",
    "        elif 'foundry' in source_lower:\n",
    "            self.conversation_stats['foundry_responses'] += 1\n",
    "        elif 'azure' in source_lower:\n",
    "            self.conversation_stats['azure_responses'] += 1\n",
    "        elif 'mock' in source_lower:\n",
    "            self.conversation_stats['mock_responses'] += 1\n",
    "        \n",
    "        # Track model switches\n",
    "        if self.conversation_stats['last_source'] and self.conversation_stats['last_source'] != source:\n",
    "            self.conversation_stats['model_switches'] += 1\n",
    "        \n",
    "        # Track fallback usage\n",
    "        if was_fallback:\n",
    "            self.conversation_stats['fallback_uses'] += 1\n",
    "        \n",
    "        self.conversation_stats['last_source'] = source\n",
    "\n",
    "    def chat(self, user_message: str, show_routing=False) -> str:\n",
    "        \"\"\"Process user message through hybrid routing with conversation context.\"\"\"\n",
    "        \n",
    "        # Use mock response if router is not available\n",
    "        if not self.router_available:\n",
    "            if show_routing:\n",
    "                print(\"üéØ Using mock routing (router unavailable)\")\n",
    "            \n",
    "            mock_response = self._create_mock_response(user_message)\n",
    "            \n",
    "            # Extract source information from mock response\n",
    "            if '[MOCK-' in mock_response:\n",
    "                source_start = mock_response.find('[MOCK-') + 1\n",
    "                source_end = mock_response.find(']', source_start)\n",
    "                source = mock_response[source_start:source_end]\n",
    "                clean_response = mock_response[source_end+1:].strip()\n",
    "            else:\n",
    "                source = \"mock-unknown\"\n",
    "                clean_response = mock_response\n",
    "            \n",
    "            # Update statistics\n",
    "            self._update_stats(source, was_fallback=True)\n",
    "            \n",
    "            # Store in history\n",
    "            exchange = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'user_message': user_message,\n",
    "                'response': clean_response,\n",
    "                'source': source,\n",
    "                'was_fallback': True,\n",
    "                'exchange_number': len(self.chat_history) + 1\n",
    "            }\n",
    "            \n",
    "            self.chat_history.append(exchange)\n",
    "            return mock_response\n",
    "        \n",
    "        # Add context-aware routing hints\n",
    "        context_info = \"\"\n",
    "        if len(self.chat_history) > 0:\n",
    "            context_info = f\" [Context: {len(self.chat_history)} previous exchanges]\"\n",
    "            user_message_with_context = f\"{user_message}{context_info}\"\n",
    "        else:\n",
    "            user_message_with_context = user_message\n",
    "        \n",
    "        # Route and get response with error handling\n",
    "        try:\n",
    "            response = self.hybrid_router.route(user_message_with_context, show_reasoning=show_routing)\n",
    "            \n",
    "            # Extract source information\n",
    "            source = \"unknown\"\n",
    "            clean_response = response\n",
    "            was_fallback = False\n",
    "            \n",
    "            # Parse response format\n",
    "            if response.startswith('[') and ']' in response:\n",
    "                source_end = response.find(']')\n",
    "                if source_end != -1:\n",
    "                    source = response[1:source_end]\n",
    "                    clean_response = response[source_end+1:].strip()\n",
    "            elif response.startswith('ERROR') or 'error' in response.lower():\n",
    "                # Handle error responses\n",
    "                source = \"ERROR\"\n",
    "                was_fallback = True\n",
    "                clean_response = \"I apologize, but I'm experiencing technical difficulties. Please try again or rephrase your question.\"\n",
    "            \n",
    "            # Detect if fallback was used\n",
    "            if not was_fallback:\n",
    "                was_fallback = '*' in source or 'fallback' in source.lower() or 'error' in source.lower()\n",
    "            \n",
    "            # Update statistics\n",
    "            self._update_stats(source, was_fallback)\n",
    "            \n",
    "            # Store in history\n",
    "            exchange = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'user_message': user_message,\n",
    "                'response': clean_response,\n",
    "                'source': source,\n",
    "                'was_fallback': was_fallback,\n",
    "                'exchange_number': len(self.chat_history) + 1\n",
    "            }\n",
    "            \n",
    "            self.chat_history.append(exchange)\n",
    "            \n",
    "            # Show routing info if requested\n",
    "            if show_routing:\n",
    "                print(f\"üéØ Routed to: {source}\")\n",
    "                if was_fallback:\n",
    "                    print(f\"üîÑ Fallback was used\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Chat error: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            # Create fallback response\n",
    "            fallback_response = f\"[ERROR] I encountered an issue processing your request. This might be due to connectivity or configuration problems. Your message was: '{user_message}'\"\n",
    "            \n",
    "            # Update statistics for error\n",
    "            self._update_stats(\"ERROR\", was_fallback=True)\n",
    "            \n",
    "            # Store error in history\n",
    "            exchange = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'user_message': user_message,\n",
    "                'response': \"I apologize, but I'm experiencing technical difficulties.\",\n",
    "                'source': \"ERROR\",\n",
    "                'was_fallback': True,\n",
    "                'exchange_number': len(self.chat_history) + 1\n",
    "            }\n",
    "            \n",
    "            self.chat_history.append(exchange)\n",
    "            return fallback_response\n",
    "    \n",
    "    def get_conversation_summary(self) -> Dict:\n",
    "        \"\"\"Get comprehensive conversation analytics.\"\"\"\n",
    "        if not self.chat_history:\n",
    "            return {\"message\": \"No conversation history available\"}\n",
    "        \n",
    "        # Calculate session duration\n",
    "        session_duration = datetime.now() - self.conversation_stats['session_start']\n",
    "        \n",
    "        # Analyze routing patterns\n",
    "        total = self.conversation_stats['total_exchanges']\n",
    "        routing_distribution = {\n",
    "            'local': (self.conversation_stats['local_responses'] / total * 100) if total > 0 else 0,\n",
    "            'apim': (self.conversation_stats['apim_responses'] / total * 100) if total > 0 else 0,\n",
    "            'foundry': (self.conversation_stats['foundry_responses'] / total * 100) if total > 0 else 0,\n",
    "            'azure': (self.conversation_stats['azure_responses'] / total * 100) if total > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'session_info': {\n",
    "                'session_id': self.session_id,\n",
    "                'duration': str(session_duration).split('.')[0],\n",
    "                'total_exchanges': total,\n",
    "                'conversation_length': len(self.chat_history)\n",
    "            },\n",
    "            'routing_stats': {\n",
    "                'distribution': routing_distribution,\n",
    "                'model_switches': self.conversation_stats['model_switches'],\n",
    "                'fallback_uses': self.conversation_stats['fallback_uses'],\n",
    "                'current_source': self.conversation_stats['last_source']\n",
    "            },\n",
    "            'conversation_flow': [\n",
    "                {\n",
    "                    'exchange': ex['exchange_number'],\n",
    "                    'source': ex['source'],\n",
    "                    'fallback': ex['was_fallback'],\n",
    "                    'timestamp': ex['timestamp'].strftime('%H:%M:%S')\n",
    "                }\n",
    "                for ex in self.chat_history[-5:]  # Last 5 exchanges\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def export_conversation(self, filename: str = None) -> str:\n",
    "        \"\"\"Export conversation to JSON file.\"\"\"\n",
    "        if not filename:\n",
    "            filename = f\"conversation_{self.session_id}.json\"\n",
    "        \n",
    "        export_data = {\n",
    "            'session_info': {\n",
    "                'session_id': self.session_id,\n",
    "                'start_time': self.conversation_stats['session_start'].isoformat(),\n",
    "                'export_time': datetime.now().isoformat(),\n",
    "                'total_exchanges': len(self.chat_history)\n",
    "            },\n",
    "            'conversation': [\n",
    "                {\n",
    "                    'exchange_number': ex['exchange_number'],\n",
    "                    'timestamp': ex['timestamp'].isoformat(),\n",
    "                    'user_message': ex['user_message'],\n",
    "                    'response': ex['response'],\n",
    "                    'source': ex['source'],\n",
    "                    'was_fallback': ex['was_fallback']\n",
    "                }\n",
    "                for ex in self.chat_history\n",
    "            ],\n",
    "            'statistics': self.conversation_stats\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, indent=2, default=str)\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def clear_conversation(self):\n",
    "        \"\"\"Clear conversation history and reset statistics.\"\"\"\n",
    "        self.chat_history.clear()\n",
    "        self.conversation_stats = {\n",
    "            'total_exchanges': 0,\n",
    "            'local_responses': 0,\n",
    "            'apim_responses': 0,\n",
    "            'foundry_responses': 0,\n",
    "            'azure_responses': 0,\n",
    "            'model_switches': 0,\n",
    "            'fallback_uses': 0,\n",
    "            'session_start': datetime.now(),\n",
    "            'last_source': None\n",
    "        }\n",
    "        print(f\"üßπ Conversation cleared for session {self.session_id}\")\n",
    "\n",
    "# Initialize the conversation manager\n",
    "if hybrid_router:\n",
    "    conversation_manager = HybridConversationManager(hybrid_router)\n",
    "    print(\"‚úÖ Hybrid Conversation Manager ready!\")\n",
    "else:\n",
    "    conversation_manager = None\n",
    "    print(\"‚ùå Cannot initialize conversation manager without hybrid router\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbc396",
   "metadata": {},
   "source": [
    "## Step 5.3: Interactive Multi-Turn Chat Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ed4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation scenarios\n",
    "test_conversation_scenarios = [\n",
    "    # Scenario 1: Simple to Complex progression\n",
    "    {\n",
    "        \"name\": \"Simple to Complex Progression\",\n",
    "        \"messages\": [\n",
    "            \"Hi there!\",  # Should go to Local\n",
    "            \"What's the weather like?\",  # Simple, Local\n",
    "            \"Explain machine learning algorithms in detail\",  # Complex, Foundry/APIM\n",
    "            \"How would you implement this in an enterprise setting?\"  # Enterprise, APIM\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Scenario 2: Multi-turn conversation with context\n",
    "    {\n",
    "        \"name\": \"Context-Aware Conversation\",\n",
    "        \"messages\": [\n",
    "            \"I'm working on a Python project\",  # Setup context\n",
    "            \"What's the best way to handle exceptions?\",  # Technical question\n",
    "            \"Can you show me an example?\",  # Follow-up with context\n",
    "            \"How would this scale in production?\"  # Enterprise follow-up\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # Scenario 3: Mixed complexity conversation\n",
    "    {\n",
    "        \"name\": \"Mixed Complexity Chat\",\n",
    "        \"messages\": [\n",
    "            \"Hello\",  # Simple greeting\n",
    "            \"Analyze the pros and cons of microservices architecture\",  # Complex analysis\n",
    "            \"Thanks!\",  # Simple response\n",
    "            \"What about security considerations?\"  # Follow-up question\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_conversation_scenario(scenario: Dict, show_routing: bool = True):\n",
    "    \"\"\"Run a conversation scenario and display results.\"\"\"\n",
    "    if not conversation_manager:\n",
    "        print(\"‚ùå Conversation manager not available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üé≠ SCENARIO: {scenario['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clear previous conversation\n",
    "    conversation_manager.clear_conversation()\n",
    "    \n",
    "    successful_exchanges = 0\n",
    "    \n",
    "    for i, message in enumerate(scenario['messages'], 1):\n",
    "        print(f\"\\nüßë User #{i}: {message}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            response = conversation_manager.chat(message, show_routing=show_routing)\n",
    "            print(f\"ü§ñ Assistant: {response}\")\n",
    "            successful_exchanges += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in exchange {i}: {e}\")\n",
    "            print(f\"ü§ñ Assistant: I apologize, I'm having technical difficulties with that request.\")\n",
    "    \n",
    "    # Show conversation summary\n",
    "    print(f\"\\nüìä CONVERSATION SUMMARY:\")\n",
    "    try:\n",
    "        summary = conversation_manager.get_conversation_summary()\n",
    "        \n",
    "        session_info = summary['session_info']\n",
    "        routing_stats = summary['routing_stats']\n",
    "        \n",
    "        print(f\"   Duration: {session_info['duration']}\")\n",
    "        print(f\"   Successful exchanges: {successful_exchanges}/{len(scenario['messages'])}\")\n",
    "        print(f\"   Total exchanges: {session_info['total_exchanges']}\")\n",
    "        print(f\"   Model switches: {routing_stats['model_switches']}\")\n",
    "        print(f\"   Fallbacks used: {routing_stats['fallback_uses']}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Routing Distribution:\")\n",
    "        for source, percentage in routing_stats['distribution'].items():\n",
    "            if percentage > 0:\n",
    "                print(f\"   {source.capitalize()}: {percentage:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüîÑ Conversation Flow:\")\n",
    "        for flow in summary['conversation_flow']:\n",
    "            status = \"üîÑ\" if flow['fallback'] else \"‚úÖ\"\n",
    "            print(f\"   {flow['exchange']}. [{flow['timestamp']}] ‚Üí {flow['source']} {status}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error generating summary: {e}\")\n",
    "\n",
    "# Run scenarios with enhanced error handling\n",
    "if conversation_manager:\n",
    "    print(\"üöÄ Starting Enhanced Multi-Turn Conversation Testing...\")\n",
    "    \n",
    "    # Test with simplified scenarios first\n",
    "    simple_scenario = {\n",
    "        \"name\": \"Basic Connectivity Test\",\n",
    "        \"messages\": [\n",
    "            \"Hello\",\n",
    "            \"How are you?\",\n",
    "            \"Tell me about AI\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        run_conversation_scenario(simple_scenario, show_routing=True)\n",
    "        \n",
    "        # If basic test works, run the full scenarios\n",
    "        for scenario in test_conversation_scenarios:\n",
    "            run_conversation_scenario(scenario, show_routing=True)\n",
    "            time.sleep(1)  # Brief pause between scenarios\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running scenarios: {e}\")\n",
    "        print(\"üí° This might be due to router configuration issues.\")\n",
    "        print(\"   The system is now running in mock mode for demonstration.\")\n",
    "        \n",
    "        # Run with mock responses\n",
    "        print(\"\\nüé≠ Running demonstration with mock responses...\")\n",
    "        run_conversation_scenario(simple_scenario, show_routing=True)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot run conversation scenarios without conversation manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cf461",
   "metadata": {},
   "source": [
    "## Step 5.4: Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat_session():\n",
    "    \"\"\"Start an interactive chat session with hybrid routing.\"\"\"\n",
    "    if not conversation_manager:\n",
    "        print(\"‚ùå Interactive chat not available without conversation manager\")\n",
    "        return\n",
    "    \n",
    "    print(\"üé≠ HYBRID CHAT SESSION STARTED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üí° Features:\")\n",
    "    print(\"   ‚Ä¢ Three-tier routing: Local ‚Üí APIM ‚Üí Foundry Agents\")\n",
    "    print(\"   ‚Ä¢ Context preservation across model switches\")\n",
    "    print(\"   ‚Ä¢ Real-time routing decisions\")\n",
    "    print(\"   ‚Ä¢ Conversation analytics\")\n",
    "    print()\n",
    "    print(\"üéØ Commands:\")\n",
    "    print(\"   'quit' or 'exit' - End session\")\n",
    "    print(\"   'stats' - Show conversation statistics\")\n",
    "    print(\"   'history' - Show recent conversation\")\n",
    "    print(\"   'clear' - Clear conversation history\")\n",
    "    print(\"   'export' - Export conversation to file\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Start fresh conversation\n",
    "    conversation_manager.clear_conversation()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nüßë You: \").strip()\n",
    "            \n",
    "            # Handle commands\n",
    "            if user_input.lower() in ['quit', 'exit']:\n",
    "                print(\"üëã Ending chat session...\")\n",
    "                break\n",
    "            elif user_input.lower() == 'stats':\n",
    "                summary = conversation_manager.get_conversation_summary()\n",
    "                print(\"\\nüìä CONVERSATION STATISTICS:\")\n",
    "                print(f\"   Session: {summary['session_info']['session_id']}\")\n",
    "                print(f\"   Duration: {summary['session_info']['duration']}\")\n",
    "                print(f\"   Exchanges: {summary['session_info']['total_exchanges']}\")\n",
    "                print(f\"   Model switches: {summary['routing_stats']['model_switches']}\")\n",
    "                print(f\"   Current source: {summary['routing_stats']['current_source']}\")\n",
    "                continue\n",
    "            elif user_input.lower() == 'history':\n",
    "                print(\"\\nüìú RECENT CONVERSATION:\")\n",
    "                for ex in conversation_manager.chat_history[-3:]:\n",
    "                    print(f\"   üßë {ex['user_message']}\")\n",
    "                    print(f\"   ü§ñ [{ex['source']}] {ex['response'][:100]}...\")\n",
    "                continue\n",
    "            elif user_input.lower() == 'clear':\n",
    "                conversation_manager.clear_conversation()\n",
    "                print(\"üßπ Conversation history cleared\")\n",
    "                continue\n",
    "            elif user_input.lower() == 'export':\n",
    "                filename = conversation_manager.export_conversation()\n",
    "                print(f\"üíæ Conversation exported to: {filename}\")\n",
    "                continue\n",
    "            elif not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Process message through hybrid routing\n",
    "            print(\"ü§ñ Assistant: \", end=\"\", flush=True)\n",
    "            response = conversation_manager.chat(user_input, show_routing=False)\n",
    "            print(response)\n",
    "            \n",
    "            # Show routing info briefly\n",
    "            if conversation_manager.chat_history:\n",
    "                last_exchange = conversation_manager.chat_history[-1]\n",
    "                source_info = f\"[{last_exchange['source']}]\"\n",
    "                if last_exchange['was_fallback']:\n",
    "                    source_info += \" üîÑ\"\n",
    "                print(f\"   {source_info}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Chat session interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "    \n",
    "    # Final summary\n",
    "    if conversation_manager.chat_history:\n",
    "        print(\"\\nüìä FINAL SESSION SUMMARY:\")\n",
    "        summary = conversation_manager.get_conversation_summary()\n",
    "        session_info = summary['session_info']\n",
    "        routing_stats = summary['routing_stats']\n",
    "        \n",
    "        print(f\"   Duration: {session_info['duration']}\")\n",
    "        print(f\"   Total exchanges: {session_info['total_exchanges']}\")\n",
    "        print(f\"   Model switches: {routing_stats['model_switches']}\")\n",
    "        print(f\"   Fallback uses: {routing_stats['fallback_uses']}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Routing Distribution:\")\n",
    "        for source, percentage in routing_stats['distribution'].items():\n",
    "            if percentage > 0:\n",
    "                print(f\"   {source.capitalize()}: {percentage:.1f}%\")\n",
    "\n",
    "# Uncomment the line below to start interactive chat\n",
    "# interactive_chat_session()\n",
    "\n",
    "print(\"‚úÖ Interactive chat interface ready!\")\n",
    "print(\"üí° Uncomment 'interactive_chat_session()' above to start chatting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e6e59",
   "metadata": {},
   "source": [
    "## Step 5.5: Advanced Features and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced conversation analytics and optimization\n",
    "def analyze_conversation_patterns():\n",
    "    \"\"\"Analyze conversation patterns and routing effectiveness.\"\"\"\n",
    "    if not conversation_manager or not conversation_manager.chat_history:\n",
    "        print(\"‚ùå No conversation data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç CONVERSATION PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    history = conversation_manager.chat_history\n",
    "    total_exchanges = len(history)\n",
    "    \n",
    "    # Routing pattern analysis\n",
    "    routing_sequences = []\n",
    "    for i in range(len(history)):\n",
    "        if i == 0:\n",
    "            routing_sequences.append(f\"START ‚Üí {history[i]['source']}\")\n",
    "        else:\n",
    "            routing_sequences.append(f\"{history[i-1]['source']} ‚Üí {history[i]['source']}\")\n",
    "    \n",
    "    print(f\"üìä Pattern Analysis:\")\n",
    "    print(f\"   Total exchanges: {total_exchanges}\")\n",
    "    \n",
    "    # Calculate routing efficiency\n",
    "    successful_primary_routes = sum(1 for ex in history if not ex['was_fallback'])\n",
    "    efficiency = (successful_primary_routes / total_exchanges * 100) if total_exchanges > 0 else 0\n",
    "    \n",
    "    print(f\"   Primary routing success: {successful_primary_routes}/{total_exchanges} ({efficiency:.1f}%)\")\n",
    "    \n",
    "    # Most common routing patterns\n",
    "    from collections import Counter\n",
    "    pattern_counts = Counter(routing_sequences)\n",
    "    print(f\"\\nüîÑ Most Common Routing Patterns:\")\n",
    "    for pattern, count in pattern_counts.most_common(3):\n",
    "        print(f\"   {pattern}: {count} times\")\n",
    "    \n",
    "    # Response time analysis (if available)\n",
    "    sources = [ex['source'] for ex in history]\n",
    "    source_counts = Counter(sources)\n",
    "    print(f\"\\nüéØ Source Distribution:\")\n",
    "    for source, count in source_counts.most_common():\n",
    "        percentage = (count / total_exchanges * 100)\n",
    "        print(f\"   {source}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Context effectiveness\n",
    "    context_switches = conversation_manager.conversation_stats['model_switches']\n",
    "    print(f\"\\nüîÄ Context Management:\")\n",
    "    print(f\"   Model switches: {context_switches}\")\n",
    "    print(f\"   Switch rate: {(context_switches / max(total_exchanges-1, 1) * 100):.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'total_exchanges': total_exchanges,\n",
    "        'routing_efficiency': efficiency,\n",
    "        'pattern_distribution': dict(pattern_counts),\n",
    "        'source_distribution': dict(source_counts),\n",
    "        'context_switches': context_switches\n",
    "    }\n",
    "\n",
    "def generate_conversation_report():\n",
    "    \"\"\"Generate comprehensive conversation report.\"\"\"\n",
    "    if not conversation_manager:\n",
    "        print(\"‚ùå No conversation manager available\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìã COMPREHENSIVE CONVERSATION REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic session info\n",
    "    summary = conversation_manager.get_conversation_summary()\n",
    "    session_info = summary['session_info']\n",
    "    routing_stats = summary['routing_stats']\n",
    "    \n",
    "    print(f\"üÜî Session Information:\")\n",
    "    print(f\"   Session ID: {session_info['session_id']}\")\n",
    "    print(f\"   Duration: {session_info['duration']}\")\n",
    "    print(f\"   Start Time: {conversation_manager.conversation_stats['session_start'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"   Total Exchanges: {session_info['total_exchanges']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Routing Performance:\")\n",
    "    print(f\"   Model Switches: {routing_stats['model_switches']}\")\n",
    "    print(f\"   Fallback Uses: {routing_stats['fallback_uses']}\")\n",
    "    print(f\"   Current Source: {routing_stats['current_source']}\")\n",
    "    \n",
    "    print(f\"\\nüìä Distribution Analysis:\")\n",
    "    for source, percentage in routing_stats['distribution'].items():\n",
    "        if percentage > 0:\n",
    "            print(f\"   {source.capitalize()}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Advanced analysis\n",
    "    if conversation_manager.chat_history:\n",
    "        analysis = analyze_conversation_patterns()\n",
    "        \n",
    "        print(f\"\\n‚ö° Efficiency Metrics:\")\n",
    "        print(f\"   Primary Routing Success: {analysis['routing_efficiency']:.1f}%\")\n",
    "        switch_rate = (analysis['context_switches'] / max(analysis['total_exchanges']-1, 1) * 100)\n",
    "        print(f\"   Context Switch Rate: {switch_rate:.1f}%\")\n",
    "        \n",
    "    print(f\"\\n‚úÖ Report generated successfully!\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test the analytics functions\n",
    "if conversation_manager and conversation_manager.chat_history:\n",
    "    print(\"üìä Running conversation analytics...\")\n",
    "    analyze_conversation_patterns()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    generate_conversation_report()\n",
    "else:\n",
    "    print(\"üí° Run some conversations first to see analytics in action!\")\n",
    "    print(\"   Use the test scenarios or interactive chat above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8408be",
   "metadata": {},
   "source": [
    "## Step 5.6: Testing ConversationContextManager Alignment\n",
    "\n",
    "**Purpose:** Test the enhanced ConversationContextManager to ensure it's aligned with HybridConversationManager and maintains conversation continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced ConversationContextManager\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import the enhanced context manager\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from modules.context_manager import ConversationContextManager\n",
    "\n",
    "print(\"üß™ Testing ConversationContextManager Alignment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a test context manager\n",
    "test_context_mgr = ConversationContextManager(\"test_session_001\")\n",
    "\n",
    "# Test conversation continuity with multiple exchanges\n",
    "print(\"\\nüîÑ Testing Conversation Continuity:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Exchange 1: Simple greeting\n",
    "exchange1 = test_context_mgr.add_exchange(\n",
    "    user_message=\"Hello! I'm new here.\",\n",
    "    ai_response=\"Welcome! I'm here to help you with any questions you have.\",\n",
    "    source=\"local\",\n",
    "    response_time=0.15,\n",
    "    metadata={\"context_used\": False, \"greeting_detected\": True}\n",
    ")\n",
    "\n",
    "print(f\"Exchange 1: ‚úÖ Added (source: {exchange1['source']}, time: {exchange1['response_time']:.3f}s)\")\n",
    "\n",
    "# Exchange 2: Follow-up question  \n",
    "exchange2 = test_context_mgr.add_exchange(\n",
    "    user_message=\"What can you help me with?\",\n",
    "    ai_response=\"I can assist with various tasks including analysis, coding, explanations, and more. What specific area are you interested in?\",\n",
    "    source=\"local\", \n",
    "    response_time=0.12,\n",
    "    metadata={\"context_used\": True, \"follow_up\": True}\n",
    ")\n",
    "\n",
    "print(f\"Exchange 2: ‚úÖ Added (source: {exchange2['source']}, time: {exchange2['response_time']:.3f}s)\")\n",
    "\n",
    "# Exchange 3: Complex request (should route to different model)\n",
    "exchange3 = test_context_mgr.add_exchange(\n",
    "    user_message=\"Can you analyze the performance implications of different database indexing strategies for a large-scale e-commerce platform?\",\n",
    "    ai_response=\"This is a complex database architecture question. For large-scale e-commerce platforms, indexing strategy significantly impacts performance...\",\n",
    "    source=\"foundry\",\n",
    "    response_time=2.34,\n",
    "    metadata={\"context_used\": True, \"complex_analysis\": True, \"routing_reason\": \"Complex technical analysis\"}\n",
    ")\n",
    "\n",
    "print(f\"Exchange 3: ‚úÖ Added (source: {exchange3['source']}, time: {exchange3['response_time']:.3f}s)\")\n",
    "\n",
    "# Exchange 4: Follow-up on previous complex topic\n",
    "exchange4 = test_context_mgr.add_exchange(\n",
    "    user_message=\"How would you implement that in a microservices architecture?\", \n",
    "    ai_response=\"In a microservices architecture, database indexing becomes more complex due to data distribution. Here's how you'd approach it...\",\n",
    "    source=\"apim\",\n",
    "    response_time=1.89,\n",
    "    metadata={\"context_used\": True, \"microservices_context\": True, \"enterprise_query\": True}\n",
    ")\n",
    "\n",
    "print(f\"Exchange 4: ‚úÖ Added (source: {exchange4['source']}, time: {exchange4['response_time']:.3f}s)\")\n",
    "\n",
    "# Exchange 5: Simple follow-up\n",
    "exchange5 = test_context_mgr.add_exchange(\n",
    "    user_message=\"Thanks, that was very helpful!\",\n",
    "    ai_response=\"You're welcome! I'm glad I could help explain the database indexing strategies for your use case.\",\n",
    "    source=\"local\",\n",
    "    response_time=0.08,\n",
    "    metadata={\"context_used\": True, \"gratitude\": True}\n",
    ")\n",
    "\n",
    "print(f\"Exchange 5: ‚úÖ Added (source: {exchange5['source']}, time: {exchange5['response_time']:.3f}s)\")\n",
    "\n",
    "print(f\"\\nüìä Conversation Statistics:\")\n",
    "print(f\"   Total exchanges: {len(test_context_mgr.chat_history)}\")\n",
    "print(f\"   Model switches: {test_context_mgr.conversation_stats['model_switches']}\")\n",
    "print(f\"   Current source: {test_context_mgr.conversation_stats['last_model_used']}\")\n",
    "\n",
    "# Test conversation context generation\n",
    "recent_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! I'm new here.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Welcome! I'm here to help you with any questions you have.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What can you help me with?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I can assist with various tasks including analysis, coding, explanations, and more.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you analyze database indexing strategies?\"}\n",
    "]\n",
    "\n",
    "context = test_context_mgr.get_conversation_context(recent_messages)\n",
    "print(f\"\\nüß† Generated Context Length: {len(context)} characters\")\n",
    "print(f\"Context Preview: {context[:200]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ ConversationContextManager test completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test session summary and conversation flow alignment with lab5 HybridConversationManager\n",
    "print(\"üîç Testing Session Summary Alignment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get session summary in lab5 format\n",
    "session_summary = test_context_mgr.get_session_summary()\n",
    "\n",
    "print(\"üìã Session Information:\")\n",
    "session_info = session_summary['session_info']\n",
    "for key, value in session_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nüéØ Routing Statistics:\")\n",
    "routing_stats = session_summary['routing_stats']\n",
    "for key, value in routing_stats.items():\n",
    "    if key == 'distribution':\n",
    "        print(f\"   {key}:\")\n",
    "        for source, percentage in value.items():\n",
    "            if percentage > 0:\n",
    "                print(f\"     {source}: {percentage:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nüîÑ Conversation Flow (Recent):\")\n",
    "for flow in session_summary['conversation_flow']:\n",
    "    status = \"üîÑ\" if flow['fallback'] else \"‚úÖ\"\n",
    "    print(f\"   {flow['exchange']}. [{flow['timestamp']}] ‚Üí {flow['source']} {status}\")\n",
    "\n",
    "# Test conversation continuity by simulating context usage\n",
    "print(f\"\\nüß† Testing Context Continuity:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get recent exchanges for context\n",
    "recent_exchanges = test_context_mgr.get_recent_exchanges(3)\n",
    "print(f\"Recent exchanges count: {len(recent_exchanges)}\")\n",
    "\n",
    "for i, exchange in enumerate(recent_exchanges, 1):\n",
    "    context_used = \"üß†\" if exchange['metadata'].get('context_used') else \"‚ùå\"\n",
    "    print(f\"   {i}. [{exchange['timestamp'].strftime('%H:%M:%S')}] {exchange['source']} {context_used}\")\n",
    "    print(f\"      User: {exchange['user_message'][:50]}...\")\n",
    "    print(f\"      AI: {exchange['response'][:50]}...\")\n",
    "\n",
    "# Test that conversation history maintains proper order and continuity\n",
    "print(f\"\\nüîó Conversation Continuity Check:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for i in range(1, len(test_context_mgr.chat_history)):\n",
    "    current = test_context_mgr.chat_history[i]\n",
    "    previous = test_context_mgr.chat_history[i-1]\n",
    "    \n",
    "    # Check if context was properly used in follow-up questions\n",
    "    if current['metadata'].get('context_used') and i > 1:\n",
    "        print(f\"   Exchange {current['exchange_number']}: Context used ‚úÖ\")\n",
    "        if current['source'] != previous['source']:\n",
    "            print(f\"     ‚Üí Model switch: {previous['source']} ‚Üí {current['source']}\")\n",
    "    \n",
    "# Test export functionality for persistence\n",
    "print(f\"\\nüíæ Testing Export Functionality:\")\n",
    "export_file = f\"test_conversation_{test_context_mgr.session_id}.json\"\n",
    "filename = test_context_mgr.export_conversation(export_file)\n",
    "print(f\"   Exported to: {filename}\")\n",
    "\n",
    "# Verify export content\n",
    "import json\n",
    "try:\n",
    "    with open(filename, 'r') as f:\n",
    "        exported_data = json.load(f)\n",
    "    \n",
    "    print(f\"   ‚úÖ Export successful!\")\n",
    "    print(f\"   - Session ID: {exported_data['session_info']['session_id']}\")\n",
    "    print(f\"   - Total exchanges: {exported_data['session_info']['total_exchanges']}\")\n",
    "    print(f\"   - Conversation entries: {len(exported_data['conversation'])}\")\n",
    "    \n",
    "    # Clean up test file\n",
    "    os.remove(filename)\n",
    "    print(f\"   üßπ Cleanup: Test file removed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Export verification failed: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Session summary and continuity tests completed!\")\n",
    "print(f\"üéØ ConversationContextManager is aligned with lab5 HybridConversationManager pattern!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a220a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation continuity with backend API format\n",
    "print(\"üîó Testing Backend API Integration and Conversation Continuity\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Test that the ConversationContextManager works with the backend API format\n",
    "test_backend_mgr = ConversationContextManager(\"backend_test_session\")\n",
    "\n",
    "# Simulate API exchanges like the backend would use them\n",
    "print(\"ü§ñ Simulating Backend API Exchanges:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Exchange 1: Initial greeting\n",
    "user_msg1 = \"Hello, I need help with my project\"\n",
    "ai_response1 = \"Hi! I'd be happy to help you with your project. What type of project are you working on?\"\n",
    "metadata1 = {\n",
    "    \"strategy\": \"hybrid\",\n",
    "    \"context_used\": False,\n",
    "    \"routing_decision\": \"greeting_detected\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"api_connected\": True\n",
    "}\n",
    "\n",
    "exchange1 = test_backend_mgr.add_exchange(user_msg1, ai_response1, \"local\", 0.12, metadata1)\n",
    "print(f\"‚úÖ Exchange 1: {exchange1['source']} - {exchange1['response_time']:.3f}s\")\n",
    "\n",
    "# Exchange 2: Follow-up with context\n",
    "user_msg2 = \"It's a web application for e-commerce\"\n",
    "ai_response2 = \"Great! For e-commerce web applications, there are several key areas to consider. What specific aspect would you like help with?\"\n",
    "metadata2 = {\n",
    "    \"strategy\": \"hybrid\", \n",
    "    \"context_used\": True,\n",
    "    \"routing_decision\": \"local_capability\",\n",
    "    \"confidence\": 0.88,\n",
    "    \"api_connected\": True,\n",
    "    \"context_length\": 1\n",
    "}\n",
    "\n",
    "exchange2 = test_backend_mgr.add_exchange(user_msg2, ai_response2, \"local\", 0.08, metadata2)\n",
    "print(f\"‚úÖ Exchange 2: {exchange2['source']} - {exchange2['response_time']:.3f}s\")\n",
    "\n",
    "# Exchange 3: Complex technical question (should route to different model)\n",
    "user_msg3 = \"I need to design a scalable database architecture that can handle millions of transactions per day while ensuring ACID compliance and supporting both OLTP and OLAP workloads\"\n",
    "ai_response3 = \"This is a complex database architecture challenge. For handling millions of daily transactions with ACID compliance and mixed OLTP/OLAP workloads, you'll need a sophisticated approach...\"\n",
    "metadata3 = {\n",
    "    \"strategy\": \"hybrid\",\n",
    "    \"context_used\": True, \n",
    "    \"routing_decision\": \"complex_technical_analysis\",\n",
    "    \"confidence\": 0.92,\n",
    "    \"api_connected\": True,\n",
    "    \"context_length\": 2,\n",
    "    \"enterprise_query\": True\n",
    "}\n",
    "\n",
    "exchange3 = test_backend_mgr.add_exchange(user_msg3, ai_response3, \"foundry\", 2.45, metadata3)\n",
    "print(f\"‚úÖ Exchange 3: {exchange3['source']} - {exchange3['response_time']:.3f}s\")\n",
    "\n",
    "# Exchange 4: Follow-up referencing previous context\n",
    "user_msg4 = \"How would this integrate with the e-commerce platform I mentioned earlier?\"\n",
    "ai_response4 = \"Excellent question! Integrating this database architecture with your e-commerce platform requires careful consideration of the transaction patterns...\"\n",
    "metadata4 = {\n",
    "    \"strategy\": \"hybrid\",\n",
    "    \"context_used\": True,\n",
    "    \"routing_decision\": \"context_aware_follow_up\", \n",
    "    \"confidence\": 0.89,\n",
    "    \"api_connected\": True,\n",
    "    \"context_length\": 3,\n",
    "    \"references_previous_context\": True\n",
    "}\n",
    "\n",
    "exchange4 = test_backend_mgr.add_exchange(user_msg4, ai_response4, \"apim\", 1.67, metadata4)\n",
    "print(f\"‚úÖ Exchange 4: {exchange4['source']} - {exchange4['response_time']:.3f}s\")\n",
    "\n",
    "# Test conversation context generation\n",
    "print(f\"\\nüß† Testing Context Generation for API:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Generate context like the backend API would\n",
    "recent_messages = []\n",
    "for ex in test_backend_mgr.chat_history:\n",
    "    recent_messages.append({\"role\": \"user\", \"content\": ex['user_message']})\n",
    "    recent_messages.append({\"role\": \"assistant\", \"content\": ex['response']})\n",
    "\n",
    "context = test_backend_mgr.get_conversation_context(recent_messages[-6:], max_context_length=500)  # Last 3 exchanges\n",
    "print(f\"Context length: {len(context)} characters\")\n",
    "print(f\"Context content:\\n{context[:300]}...\")\n",
    "\n",
    "# Test session summary in backend-compatible format\n",
    "print(f\"\\nüìä Session Summary (Backend Compatible):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "summary = test_backend_mgr.get_session_summary()\n",
    "print(f\"Session ID: {summary['session_info']['session_id']}\")\n",
    "print(f\"Total exchanges: {summary['session_info']['total_exchanges']}\")\n",
    "print(f\"Model switches: {summary['routing_stats']['model_switches']}\")\n",
    "\n",
    "# Verify conversation continuity \n",
    "print(f\"\\nüîó Conversation Continuity Verification:\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "for i, ex in enumerate(test_backend_mgr.chat_history):\n",
    "    context_indicator = \"üß†\" if ex['metadata'].get('context_used') else \"‚ùå\"\n",
    "    switch_indicator = \"üîÑ\" if i > 0 and test_backend_mgr.chat_history[i-1]['source'] != ex['source'] else \"\"\n",
    "    \n",
    "    print(f\"   {ex['exchange_number']}. [{ex['source']}] {context_indicator} {switch_indicator}\")\n",
    "    print(f\"      Query: {ex['user_message'][:60]}...\")\n",
    "    \n",
    "    # Check if the exchange properly references previous context\n",
    "    if ex['metadata'].get('references_previous_context'):\n",
    "        print(f\"      üîó References previous context\")\n",
    "    if ex['metadata'].get('context_length', 0) > 0:\n",
    "        print(f\"      üìä Context length: {ex['metadata']['context_length']} exchanges\")\n",
    "\n",
    "print(f\"\\n‚úÖ Backend integration test completed!\")\n",
    "print(f\"üéØ ConversationContextManager maintains proper conversation continuity!\")\n",
    "print(f\"üîÑ Model switches are tracked correctly: {test_backend_mgr.conversation_stats['model_switches']}\")\n",
    "\n",
    "# Test that conversation history is preserved properly\n",
    "print(f\"\\nüìù Final Continuity Check:\")\n",
    "print(f\"   - Conversation flows from greeting ‚Üí follow-up ‚Üí complex analysis ‚Üí context-aware follow-up\")\n",
    "print(f\"   - Context is properly maintained across model switches\")  \n",
    "print(f\"   - Metadata preserves API-specific information\")\n",
    "print(f\"   - Session management works for multi-turn conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afb4c8",
   "metadata": {},
   "source": [
    "## üéâ Lab 5 Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ‚úÖ **Advanced Hybrid Orchestration**: Integrated three-tier routing (Local ‚Üí APIM ‚Üí Foundry Agents) with conversation management\n",
    "- ‚úÖ **Context Preservation**: Maintained chat history across all routing targets and model switches\n",
    "- ‚úÖ **Multi-Turn Conversations**: Seamless context-aware conversations with intelligent routing\n",
    "- ‚úÖ **Real-Time Analytics**: Comprehensive conversation tracking and performance analysis\n",
    "- ‚úÖ **Interactive Interface**: Full-featured chat system with commands and session management\n",
    "\n",
    "### Key Features Implemented:\n",
    "\n",
    "**üé≠ Hybrid Conversation Manager:**\n",
    "- Three-tier routing integration with conversation context\n",
    "- Automatic context preservation across model switches\n",
    "- Real-time routing statistics and performance tracking\n",
    "- Session management with export capabilities\n",
    "\n",
    "**ü§ñ Intelligent Routing:**\n",
    "- Context-aware routing decisions based on conversation history\n",
    "- Fallback chain management with transparency\n",
    "- Source tracking and routing pattern analysis\n",
    "- Enterprise-grade APIM integration for business queries\n",
    "\n",
    "**üìä Advanced Analytics:**\n",
    "- Conversation pattern analysis and routing effectiveness\n",
    "- Real-time performance metrics and efficiency tracking\n",
    "- Export capabilities for conversation data\n",
    "- Comprehensive reporting with detailed insights\n",
    "\n",
    "**üí¨ Enterprise Chat Features:**\n",
    "- Interactive chat interface with command system\n",
    "- Session persistence and conversation export\n",
    "- Multi-scenario testing with automated analysis\n",
    "- Production-ready conversation management\n",
    "\n",
    "### Architecture Benefits:\n",
    "\n",
    "üèóÔ∏è **Scalable Design**: Modular architecture supports enterprise deployment  \n",
    "üîÑ **Seamless Switching**: Transparent model changes maintain conversation flow  \n",
    "üìà **Analytics-Driven**: Real-time metrics enable continuous optimization  \n",
    "üõ°Ô∏è **Production-Ready**: Robust error handling and fallback mechanisms  \n",
    "üí∞ **Cost-Effective**: Intelligent routing optimizes resource usage  \n",
    "\n",
    "### Next Steps:\n",
    "- **Lab 6**: Observability and telemetry integration\n",
    "- **Lab 7**: Frontend chat interface development\n",
    "- **Production**: Deploy with full monitoring and analytics\n",
    "\n",
    "**Your enterprise-grade hybrid orchestration system with advanced conversation management is complete!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
