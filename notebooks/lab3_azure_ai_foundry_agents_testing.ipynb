{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2cfc5f5",
   "metadata": {},
   "source": [
    "# Lab 3 (Alternate): Azure AI Foundry Agents Integration\n",
    "\n",
    "**Purpose:** Learn how to use Azure AI Foundry Agents for sophisticated chat completion and agent-based interactions. This lab demonstrates how to leverage Azure AI Foundry's agent capabilities for more advanced conversational AI scenarios beyond simple chat completions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this alternate Lab 3, we'll:\n",
    "- Connect to Azure AI Foundry using the Agents SDK\n",
    "- Create and configure AI agents with specific capabilities\n",
    "- Test agent-based interactions for complex tasks\n",
    "- Compare agent performance with direct model calls\n",
    "- Understand when to use agents vs direct model access in our hybrid architecture\n",
    "\n",
    "## Azure AI Foundry Agents Benefits\n",
    "- **Structured Conversations**: Built-in conversation management\n",
    "- **Tool Integration**: Native support for function calling and tools\n",
    "- **State Management**: Automatic handling of conversation state\n",
    "- **Advanced Capabilities**: Multi-turn reasoning, planning, and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b446fa",
   "metadata": {},
   "source": [
    "## Step 3.1: Environment Setup and Azure AI Foundry Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb86360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for module imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Standard library imports\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load configuration\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment setup complete\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundry_local import FoundryLocalManager\n",
    "\n",
    "# Initialize and optionally bootstrap with a model\n",
    "manager = FoundryLocalManager(alias_or_model_id=None, bootstrap=True)\n",
    "\n",
    "LOCAL_ENDPOINT = manager.service_uri\n",
    "LOCAL_MODEL_NAME = os.environ[\"LOCAL_MODEL_NAME\"]\n",
    "AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "print(f\"Local service: {LOCAL_ENDPOINT}\")\n",
    "print(f\"Local endpoint: {manager.endpoint}\")\n",
    "print(f\"Local model alias: {LOCAL_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Foundry and Agents configuration\n",
    "AZURE_AI_FOUNDRY_ENDPOINT = os.environ[\"AZURE_AI_FOUNDRY_ENDPOINT\"]\n",
    "AZURE_AI_FOUNDRY_PROJECT_ENDPOINT = os.environ[\"AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\"]\n",
    "\n",
    "# Azure OpenAI configuration (direct)\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_KEY = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "AZURE_OPENAI_DEPLOYMENT = os.environ[\"AZURE_DEPLOYMENT_NAME\"]\n",
    "AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "\n",
    "print(\"ğŸ”§ Configuration Summary:\")\n",
    "print(f\"   Azure AI Project: {'âœ… Configured' if AZURE_AI_FOUNDRY_ENDPOINT else 'âŒ Missing'}\")\n",
    "print(f\"   Azure Foundry Project Endpoint: {AZURE_AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "print(f\"   Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"   Azure Deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"   API Version: {AZURE_OPENAI_API_VERSION}\")\n",
    "print(f\"   Local Endpoint: {LOCAL_ENDPOINT}\")\n",
    "print(f\"   Local Model: {LOCAL_MODEL_NAME}\")\n",
    "\n",
    "# Verify required configuration\n",
    "required_vars = [AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, AZURE_OPENAI_DEPLOYMENT]\n",
    "if not all(required_vars):\n",
    "    print(\"\\nâŒ Missing required Azure configuration. Please check your .env file.\")\n",
    "    print(\"Required variables: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, AZURE_DEPLOYMENT_NAME\")\n",
    "else:\n",
    "    print(\"\\nâœ… All required Azure configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5673480",
   "metadata": {},
   "source": [
    "## Step 3.2: Install and Import Azure AI Foundry Agents SDK\n",
    "\n",
    "We'll use the Azure AI Foundry Agents SDK for creating and managing AI agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12238905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure AI Foundry Agents and related libraries\n",
    "try:\n",
    "    from azure.ai.projects import AIProjectClient\n",
    "    from azure.ai.agents.models import (\n",
    "        CodeInterpreterTool\n",
    "    )\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    from azure.ai.inference import ChatCompletionsClient\n",
    "    from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "    print(\"âœ… Azure AI Foundry Agents SDK imported successfully\")\n",
    "    foundry_agents_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Azure AI Foundry Agents SDK not available: {e}\")\n",
    "    print(\"   Falling back to direct Azure OpenAI client\")\n",
    "    foundry_agents_available = False\n",
    "\n",
    "# Also import direct Azure OpenAI client as fallback\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "print(\"âœ… Import setup complete\")\n",
    "print(f\"   Foundry Agents available: {foundry_agents_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15484d9",
   "metadata": {},
   "source": [
    "## Step 3.3: Initialize Azure AI Foundry Project Client\n",
    "\n",
    "Set up the Azure AI Foundry project client for agent management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure AI Foundry Project Client\n",
    "project_client = None\n",
    "azure_client = None\n",
    "\n",
    "if foundry_agents_available and AZURE_AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    try:\n",
    "        # Try to initialize with project connection string\n",
    "        credential = DefaultAzureCredential()\n",
    "        project_client = AIProjectClient(\n",
    "            endpoint=AZURE_AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            credential=credential,  # Use Azure Default Credential for\n",
    "                                                # authentication\n",
    "        )\n",
    "        print(\"âœ… Azure AI Foundry Project Client initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to initialize Project Client: {e}\")\n",
    "        print(\"   Will use direct Azure OpenAI client instead\")\n",
    "        project_client = None\n",
    "\n",
    "# Initialize direct Azure OpenAI client as fallback\n",
    "try:\n",
    "    azure_client = AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "    )\n",
    "    print(\"âœ… Azure OpenAI client initialized (fallback)\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to initialize Azure OpenAI client: {e}\")\n",
    "\n",
    "# Determine which approach to use\n",
    "use_foundry_agents = project_client is not None\n",
    "use_direct_openai = azure_client is not None\n",
    "\n",
    "print(f\"\\nğŸ¯ Available Approaches:\")\n",
    "print(f\"   Azure AI Foundry Agents: {'âœ… Available' if use_foundry_agents else 'âŒ Not available'}\")\n",
    "print(f\"   Direct Azure OpenAI: {'âœ… Available' if use_direct_openai else 'âŒ Not available'}\")\n",
    "\n",
    "if use_foundry_agents:\n",
    "    print(\"\\nğŸš€ Will use Azure AI Foundry Agents for advanced capabilities\")\n",
    "elif use_direct_openai:\n",
    "    print(\"\\nğŸ”„ Will use direct Azure OpenAI client\")\n",
    "else:\n",
    "    print(\"\\nâŒ No Azure services available. Please check configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fef29",
   "metadata": {},
   "source": [
    "## Step 3.4: Create Azure AI Foundry Agent\n",
    "\n",
    "If Azure AI Foundry Agents are available, we'll create a specialized agent for our tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure AI Foundry Agent (if available)\n",
    "foundry_agent = None\n",
    "agent_thread = None\n",
    "\n",
    "if use_foundry_agents:\n",
    "    try:\n",
    "        # Create an agent with specific instructions and capabilities\n",
    "        agent_instructions = \"\"\"\n",
    "        You are an intelligent AI assistant specialized in hybrid AI system analysis and documentation.\n",
    "        Your primary capabilities include:\n",
    "        1. Document analysis and summarization\n",
    "        2. Technical explanation and education\n",
    "        3. Business analysis and strategic recommendations\n",
    "        4. Creative content generation\n",
    "        5. Complex reasoning and problem-solving\n",
    "        \n",
    "        When responding:\n",
    "        - Provide clear, well-structured answers\n",
    "        - Include relevant examples when helpful\n",
    "        - Acknowledge the complexity level of requests\n",
    "        - Suggest when a task might be better suited for local vs cloud processing\n",
    "        \n",
    "        You are designed to handle sophisticated queries that require deep analysis,\n",
    "        creativity, or extensive reasoning capabilities.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create agent\n",
    "        foundry_agent = project_client.agents.create_agent(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            name=\"Hybrid-AI-Specialist\",\n",
    "            instructions=agent_instructions.strip(),\n",
    "            description=\"Specialized agent for hybrid AI system analysis and complex tasks\"\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Azure AI Foundry Agent created:\")\n",
    "        print(f\"   Agent ID: {foundry_agent.id}\")\n",
    "        print(f\"   Agent Name: {foundry_agent.name}\")\n",
    "        print(f\"   Model: {foundry_agent.model}\")\n",
    "        \n",
    "        # Create a conversation thread\n",
    "        agent_thread = project_client.agents.threads.create()\n",
    "        print(f\"   Thread ID: {agent_thread.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create Foundry Agent: {e}\")\n",
    "        print(\"   Will use direct Azure OpenAI approach\")\n",
    "        use_foundry_agents = False\n",
    "        foundry_agent = None\n",
    "        agent_thread = None\n",
    "\n",
    "else:\n",
    "    print(\"ğŸ”„ Using direct Azure OpenAI client (Foundry Agents not available)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Agent Setup Status:\")\n",
    "print(f\"   Foundry Agent: {'âœ… Ready' if foundry_agent else 'âŒ Not available'}\")\n",
    "print(f\"   Direct Client: {'âœ… Ready' if azure_client else 'âŒ Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed891d",
   "metadata": {},
   "source": [
    "## Step 3.5: Create Unified Query Functions\n",
    "\n",
    "Let's create functions that can use either Azure AI Foundry Agents or direct Azure OpenAI, depending on availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fa732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_foundry_agent(prompt: str, thread_id: str = None) -> tuple:\n",
    "    \"\"\"Query using Azure AI Foundry Agent.\"\"\"\n",
    "    if not foundry_agent or not project_client:\n",
    "        return \"Foundry Agent not available\", 0, False\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use existing thread or create new one\n",
    "        if thread_id is None:\n",
    "            current_thread = agent_thread\n",
    "        else:\n",
    "            current_thread = project_client.agents.threads.get(thread_id)\n",
    "        \n",
    "        # Add message to thread\n",
    "        message = project_client.agents.messages.create(\n",
    "            thread_id=current_thread.id,\n",
    "            role=\"user\",\n",
    "            content=prompt\n",
    "        )\n",
    "\n",
    "        print(f\"Created message, ID: {message.id}\")\n",
    "        \n",
    "        # Create and wait for run\n",
    "        run = project_client.agents.runs.create_and_process(\n",
    "            thread_id=current_thread.id,\n",
    "            agent_id=foundry_agent.id\n",
    "        )\n",
    "\n",
    "        print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "        # Check if the run failed\n",
    "        if run.status == \"failed\":\n",
    "            print(f\"Run failed: {run.last_error}\")\n",
    "            return f\"Run failed: {run.last_error}\", time.time() - start_time, False\n",
    "\n",
    "        # Wait for completion (additional safety check)\n",
    "        while run.status in [\"in_progress\", \"queued\"]:\n",
    "            time.sleep(0.5)\n",
    "            run = project_client.agents.runs.get(thread_id=current_thread.id, run_id=run.id)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status == \"completed\":\n",
    "            # Get the latest message\n",
    "            messages = project_client.agents.messages.list(thread_id=current_thread.id)\n",
    "            \n",
    "            # Convert ItemPaged to list and get the most recent message\n",
    "            message_list = list(messages)\n",
    "            if message_list:\n",
    "                latest_message = message_list[0]  # Most recent message\n",
    "                \n",
    "                if latest_message.role == \"assistant\":\n",
    "                    # Handle different content types\n",
    "                    if hasattr(latest_message.content[0], 'text'):\n",
    "                        content = latest_message.content[0].text.value\n",
    "                    else:\n",
    "                        content = str(latest_message.content[0])\n",
    "                    return content, end_time - start_time, True\n",
    "                else:\n",
    "                    return \"No assistant response found\", end_time - start_time, False\n",
    "            else:\n",
    "                return \"No messages found in thread\", end_time - start_time, False\n",
    "        else:\n",
    "            return f\"Run failed with status: {run.status}\", end_time - start_time, False\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Foundry Agent error: {str(e)}\", 0, False\n",
    "\n",
    "def query_with_direct_openai(prompt: str, chat_history: list = None, max_tokens: int = 500) -> tuple:\n",
    "    \"\"\"Query using direct Azure OpenAI client.\"\"\"\n",
    "    if not azure_client:\n",
    "        return \"Azure OpenAI client not available\", 0, False\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare messages\n",
    "        if chat_history is None:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        else:\n",
    "            messages = chat_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        # Make API call\n",
    "        response = azure_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        content = response.choices[0].message.content\n",
    "        return content, end_time - start_time, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Direct OpenAI error: {str(e)}\", 0, False\n",
    "\n",
    "def query_azure_intelligent(prompt: str, chat_history: list = None, max_tokens: int = 500) -> tuple:\n",
    "    \"\"\"Intelligently choose between Foundry Agent and direct OpenAI.\"\"\"\n",
    "    if use_foundry_agents:\n",
    "        print(\"ğŸ¤– Using Azure AI Foundry Agent...\")\n",
    "        return query_with_foundry_agent(prompt)\n",
    "    elif use_direct_openai:\n",
    "        print(\"â˜ï¸ Using direct Azure OpenAI...\")\n",
    "        return query_with_direct_openai(prompt, chat_history, max_tokens)\n",
    "    else:\n",
    "        return \"No Azure services available\", 0, False\n",
    "\n",
    "print(\"âœ… Query functions created\")\n",
    "print(f\"   Primary method: {'Foundry Agents' if use_foundry_agents else 'Direct OpenAI' if use_direct_openai else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf8e20",
   "metadata": {},
   "source": [
    "## Step 3.6: Test Basic Azure Connectivity\n",
    "\n",
    "Let's test our Azure connection with a simple query to ensure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic connectivity\n",
    "test_prompt = \"Hello! Please introduce yourself and describe your capabilities as an AI assistant.\"\n",
    "\n",
    "print(\"ğŸ§ª Testing Azure connectivity and agent capabilities\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test query: {test_prompt}\")\n",
    "print()\n",
    "\n",
    "response, response_time, success = query_azure_intelligent(test_prompt, max_tokens=200)\n",
    "\n",
    "if success:\n",
    "    print(f\"âœ… Connection successful!\")\n",
    "    print(f\"â±ï¸ Response time: {response_time:.3f} seconds\")\n",
    "    print(f\"ğŸ“ Response length: {len(response)} characters\")\n",
    "    print(\"\\nğŸ¤– Azure Response:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(response)\n",
    "    print(\"-\" * 40)\n",
    "else:\n",
    "    print(f\"âŒ Connection failed: {response}\")\n",
    "    print(\"Please check your Azure configuration and try again.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c668d",
   "metadata": {},
   "source": [
    "## Step 3.7: Test Complex Document Analysis\n",
    "\n",
    "Now let's test the Azure system with complex document analysis tasks that showcase agent capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample business document for analysis\n",
    "business_document = \"\"\"\n",
    "EXECUTIVE SUMMARY: HYBRID AI IMPLEMENTATION STRATEGY\n",
    "\n",
    "Our organization is evaluating the implementation of a hybrid AI architecture that combines \n",
    "on-premises local models with cloud-based AI services. This strategic initiative aims to \n",
    "optimize performance, cost, and data privacy while maximizing AI capabilities.\n",
    "\n",
    "KEY BENEFITS:\n",
    "1. Performance Optimization: Local models provide sub-second response times for simple queries,\n",
    "   while cloud models handle complex reasoning tasks.\n",
    "2. Cost Management: Reduces cloud API costs by routing simple requests locally.\n",
    "3. Data Privacy: Sensitive data can be processed locally without leaving the organization.\n",
    "4. Scalability: Cloud resources handle peak loads and complex workloads.\n",
    "5. Reliability: Local fallback ensures service continuity during network issues.\n",
    "\n",
    "IMPLEMENTATION CHALLENGES:\n",
    "- Model Management: Maintaining and updating local models requires expertise.\n",
    "- Routing Logic: Intelligent decision-making for local vs cloud routing.\n",
    "- Infrastructure: On-premises hardware requirements and maintenance.\n",
    "- Integration: Seamless user experience across hybrid architecture.\n",
    "- Monitoring: Comprehensive observability across local and cloud components.\n",
    "\n",
    "FINANCIAL PROJECTIONS:\n",
    "Initial investment: $150,000 (hardware, software, training)\n",
    "Annual operational costs: $75,000\n",
    "Projected savings: $200,000 annually in cloud API costs\n",
    "ROI timeline: 12-18 months\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "Proceed with pilot implementation focusing on customer service and document processing use cases.\n",
    "Establish success metrics and evaluation criteria before full deployment.\n",
    "Invest in team training and change management processes.\n",
    "\"\"\"\n",
    "\n",
    "# Test document analysis\n",
    "analysis_prompt = f\"\"\"\n",
    "Please analyze the following business document and provide:\n",
    "1. A concise executive summary (2-3 sentences)\n",
    "2. Key strategic recommendations\n",
    "3. Potential risks and mitigation strategies\n",
    "4. Assessment of the financial viability\n",
    "\n",
    "Document:\n",
    "{business_document}\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ“Š Testing complex document analysis with Azure AI\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Document length: {len(business_document)} characters\")\n",
    "print(f\"Word count: ~{len(business_document.split())} words\")\n",
    "print(\"\\nRequesting comprehensive analysis...\\n\")\n",
    "\n",
    "analysis, response_time, success = query_azure_intelligent(analysis_prompt, max_tokens=600)\n",
    "\n",
    "if success:\n",
    "    print(f\"âœ… Analysis completed successfully\")\n",
    "    print(f\"â±ï¸ Processing time: {response_time:.3f} seconds\")\n",
    "    print(f\"ğŸ“„ Analysis length: {len(analysis)} characters\")\n",
    "    print(\"\\nğŸ“‹ Business Document Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(analysis)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate analysis metrics\n",
    "    compression_ratio = len(business_document) / len(analysis)\n",
    "    words_per_second = len(analysis.split()) / response_time if response_time > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Analysis Metrics:\")\n",
    "    print(f\"   Compression ratio: {compression_ratio:.1f}:1\")\n",
    "    print(f\"   Processing speed: {words_per_second:.1f} words/second\")\n",
    "    print(f\"   Analysis depth: {'Comprehensive' if len(analysis) > 800 else 'Moderate' if len(analysis) > 400 else 'Brief'}\")\n",
    "else:\n",
    "    print(f\"âŒ Document analysis failed: {analysis}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612af527",
   "metadata": {},
   "source": [
    "## Step 3.8: Test Creative and Strategic Tasks\n",
    "\n",
    "Let's test the Azure system with tasks that require creativity and strategic thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ee373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative and strategic tasks\n",
    "advanced_tasks = [\n",
    "    {\n",
    "        \"name\": \"Strategic Planning\",\n",
    "        \"prompt\": \"Develop a comprehensive go-to-market strategy for a hybrid AI platform targeting enterprise customers. Include market positioning, pricing strategy, competitive analysis, and key success metrics.\",\n",
    "        \"max_tokens\": 500,\n",
    "        \"expected_elements\": [\"market\", \"pricing\", \"competitive\", \"metrics\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical Architecture\",\n",
    "        \"prompt\": \"Design a scalable architecture for a hybrid AI system that can handle 10,000 concurrent users with both local and cloud processing. Include load balancing, failover mechanisms, and performance optimization strategies.\",\n",
    "        \"max_tokens\": 450,\n",
    "        \"expected_elements\": [\"scalable\", \"load\", \"failover\", \"optimization\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Risk Assessment\",\n",
    "        \"prompt\": \"Conduct a comprehensive risk assessment for implementing hybrid AI in a financial services organization. Include regulatory, security, operational, and reputational risks with mitigation strategies.\",\n",
    "        \"max_tokens\": 400,\n",
    "        \"expected_elements\": [\"regulatory\", \"security\", \"operational\", \"mitigation\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ Testing advanced strategic and creative capabilities\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_time = 0\n",
    "successful_tasks = 0\n",
    "task_results = []\n",
    "\n",
    "for i, task in enumerate(advanced_tasks, 1):\n",
    "    print(f\"\\n{i}. {task['name']}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ğŸ“‹ Task: {task['prompt'][:100]}...\")\n",
    "    print(\"\\nğŸ”„ Processing...\")\n",
    "    \n",
    "    response, response_time, success = query_azure_intelligent(\n",
    "        task['prompt'], \n",
    "        max_tokens=task['max_tokens']\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        successful_tasks += 1\n",
    "        total_time += response_time\n",
    "        \n",
    "        # Check for expected elements\n",
    "        elements_found = sum(1 for element in task['expected_elements'] \n",
    "                           if element.lower() in response.lower())\n",
    "        completeness = elements_found / len(task['expected_elements'])\n",
    "        \n",
    "        task_results.append({\n",
    "            'name': task['name'],\n",
    "            'response_time': response_time,\n",
    "            'response_length': len(response),\n",
    "            'completeness': completeness\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ… Task completed successfully\")\n",
    "        print(f\"â±ï¸ Time: {response_time:.3f} seconds\")\n",
    "        print(f\"ğŸ“ Length: {len(response)} characters\")\n",
    "        print(f\"ğŸ¯ Completeness: {completeness:.1%} (found {elements_found}/{len(task['expected_elements'])} key elements)\")\n",
    "        print(f\"\\nğŸ¤– Response Preview:\")\n",
    "        print(response[:300] + \"...\" if len(response) > 300 else response)\n",
    "        \n",
    "    else:\n",
    "        print(f\"âŒ Task failed: {response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Summary statistics\n",
    "if successful_tasks > 0:\n",
    "    avg_response_time = total_time / successful_tasks\n",
    "    avg_completeness = sum(r['completeness'] for r in task_results) / len(task_results)\n",
    "    avg_length = sum(r['response_length'] for r in task_results) / len(task_results)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Advanced Task Performance Summary:\")\n",
    "    print(f\"   Successful tasks: {successful_tasks}/{len(advanced_tasks)} ({successful_tasks/len(advanced_tasks):.1%})\")\n",
    "    print(f\"   Average response time: {avg_response_time:.3f} seconds\")\n",
    "    print(f\"   Average completeness: {avg_completeness:.1%}\")\n",
    "    print(f\"   Average response length: {avg_length:.0f} characters\")\n",
    "    print(f\"   Total processing time: {total_time:.3f} seconds\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    if avg_response_time < 5 and avg_completeness > 0.7:\n",
    "        print(\"\\nğŸ‰ Excellent performance - ready for production workloads!\")\n",
    "    elif avg_response_time < 10 and avg_completeness > 0.5:\n",
    "        print(\"\\nâœ… Good performance - suitable for most enterprise use cases\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Performance may need optimization for production use\")\n",
    "else:\n",
    "    print(\"\\nâŒ No tasks completed successfully. Please check configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8e2f0",
   "metadata": {},
   "source": [
    "## Step 3.9: Compare Agent vs Direct API Performance\n",
    "\n",
    "If both methods are available, let's compare their performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison test queries\n",
    "comparison_queries = [\n",
    "    \"Explain the benefits of hybrid AI architectures in simple terms.\",\n",
    "    \"What are the key considerations for implementing edge AI in retail?\",\n",
    "    \"Summarize the main advantages of using local models vs cloud models.\"\n",
    "]\n",
    "\n",
    "print(\"âš–ï¸ Comparing Azure AI Foundry Agent vs Direct OpenAI Performance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if use_foundry_agents and use_direct_openai:\n",
    "    foundry_times = []\n",
    "    direct_times = []\n",
    "    \n",
    "    for i, query in enumerate(comparison_queries, 1):\n",
    "        print(f\"\\n{i}. Query: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Test with Foundry Agent\n",
    "        print(\"ğŸ¤– Testing with Foundry Agent...\")\n",
    "        foundry_response, foundry_time, foundry_success = query_with_foundry_agent(query)\n",
    "        \n",
    "        if foundry_success:\n",
    "            foundry_times.append(foundry_time)\n",
    "            print(f\"   âœ… Success ({foundry_time:.3f}s, {len(foundry_response)} chars)\")\n",
    "            print(f\"   Preview: {foundry_response[:150]}...\")\n",
    "        else:\n",
    "            print(f\"   âŒ Failed: {foundry_response}\")\n",
    "        \n",
    "        # Test with Direct OpenAI\n",
    "        print(\"\\nâ˜ï¸ Testing with Direct OpenAI...\")\n",
    "        direct_response, direct_time, direct_success = query_with_direct_openai(query, max_tokens=300)\n",
    "        \n",
    "        if direct_success:\n",
    "            direct_times.append(direct_time)\n",
    "            print(f\"   âœ… Success ({direct_time:.3f}s, {len(direct_response)} chars)\")\n",
    "            print(f\"   Preview: {direct_response[:150]}...\")\n",
    "        else:\n",
    "            print(f\"   âŒ Failed: {direct_response}\")\n",
    "        \n",
    "        # Compare if both succeeded\n",
    "        if foundry_success and direct_success:\n",
    "            speed_ratio = foundry_time / direct_time if direct_time > 0 else float('inf')\n",
    "            print(f\"\\nğŸ“Š Comparison:\")\n",
    "            print(f\"   Foundry Agent: {foundry_time:.3f}s\")\n",
    "            print(f\"   Direct OpenAI: {direct_time:.3f}s\")\n",
    "            print(f\"   Speed ratio: {speed_ratio:.2f}x {'(Agent slower)' if speed_ratio > 1 else '(Agent faster)'}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Overall comparison\n",
    "    if foundry_times and direct_times:\n",
    "        avg_foundry = sum(foundry_times) / len(foundry_times)\n",
    "        avg_direct = sum(direct_times) / len(direct_times)\n",
    "        \n",
    "        print(f\"\\nğŸ† Performance Summary:\")\n",
    "        print(f\"   Foundry Agent average: {avg_foundry:.3f} seconds\")\n",
    "        print(f\"   Direct OpenAI average: {avg_direct:.3f} seconds\")\n",
    "        \n",
    "        if avg_foundry < avg_direct:\n",
    "            improvement = ((avg_direct - avg_foundry) / avg_direct) * 100\n",
    "            print(f\"   ğŸš€ Foundry Agent is {improvement:.1f}% faster\")\n",
    "        else:\n",
    "            overhead = ((avg_foundry - avg_direct) / avg_direct) * 100\n",
    "            print(f\"   âš ï¸ Foundry Agent has {overhead:.1f}% overhead\")\n",
    "            print(f\"       (This overhead often provides additional capabilities)\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Cannot compare - only one method available\")\n",
    "    if use_foundry_agents:\n",
    "        print(\"   Available: Azure AI Foundry Agents\")\n",
    "    elif use_direct_openai:\n",
    "        print(\"   Available: Direct Azure OpenAI\")\n",
    "    else:\n",
    "        print(\"   Available: None\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65f1e7",
   "metadata": {},
   "source": [
    "## Step 3.10: Test Multi-turn Conversation Capabilities\n",
    "\n",
    "Let's test the conversation management capabilities, especially if using Azure AI Foundry Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation test\n",
    "conversation_turns = [\n",
    "    \"I'm planning to implement a hybrid AI system for my company. Can you help me get started?\",\n",
    "    \"What are the main technical considerations I should be aware of?\",\n",
    "    \"How should I approach the cost-benefit analysis?\",\n",
    "    \"Can you provide a rough timeline for implementation?\",\n",
    "    \"What success metrics should I track?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¬ Testing multi-turn conversation capabilities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if use_foundry_agents:\n",
    "    print(\"ğŸ¤– Using Azure AI Foundry Agent (with thread management)\")\n",
    "    # Create a new thread for this conversation\n",
    "    try:\n",
    "        conversation_thread = project_client.agents.threads.create()\n",
    "        print(f\"   Created conversation thread: {conversation_thread.id}\")\n",
    "        \n",
    "        for i, turn in enumerate(conversation_turns, 1):\n",
    "            print(f\"\\n{i}. ğŸ‘¤ User: {turn}\")\n",
    "            \n",
    "            response, response_time, success = query_with_foundry_agent(\n",
    "                turn, thread_id=conversation_thread.id\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                print(f\"ğŸ¤– Agent ({response_time:.3f}s): {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "            else:\n",
    "                print(f\"âŒ Agent failed: {response}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nâœ… Multi-turn conversation completed using thread {conversation_thread.id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Thread management failed: {e}\")\n",
    "        print(\"   Falling back to direct OpenAI with manual history management\")\n",
    "        use_foundry_agents = False\n",
    "\n",
    "if not use_foundry_agents and use_direct_openai:\n",
    "    print(\"â˜ï¸ Using Direct Azure OpenAI (with manual history management)\")\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    for i, turn in enumerate(conversation_turns, 1):\n",
    "        print(f\"\\n{i}. ğŸ‘¤ User: {turn}\")\n",
    "        \n",
    "        response, response_time, success = query_with_direct_openai(\n",
    "            turn, chat_history=chat_history, max_tokens=300\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"ğŸ¤– Assistant ({response_time:.3f}s): {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "            \n",
    "            # Update chat history\n",
    "            chat_history.append({\"role\": \"user\", \"content\": turn})\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            # Keep history manageable (last 8 messages)\n",
    "            if len(chat_history) > 8:\n",
    "                chat_history = chat_history[-8:]\n",
    "        else:\n",
    "            print(f\"âŒ Assistant failed: {response}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nâœ… Multi-turn conversation completed with {len(chat_history)} messages in history\")\n",
    "\n",
    "if not use_foundry_agents and not use_direct_openai:\n",
    "    print(\"âŒ No Azure services available for conversation testing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef027b",
   "metadata": {},
   "source": [
    "## Step 3.11: Performance Analysis and Recommendations\n",
    "\n",
    "Let's analyze the performance characteristics and provide recommendations for the hybrid system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis and recommendations\n",
    "print(\"ğŸ“Š Azure AI Foundry vs Direct OpenAI Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Foundry Agents analysis\n",
    "if use_foundry_agents:\n",
    "    print(\"\\nğŸ¤– Azure AI Foundry Agents - Strengths:\")\n",
    "    foundry_strengths = [\n",
    "        \"âœ… Built-in conversation state management\",\n",
    "        \"âœ… Advanced thread and message handling\",\n",
    "        \"âœ… Native tool integration capabilities\", \n",
    "        \"âœ… Structured agent instructions and behavior\",\n",
    "        \"âœ… Multi-turn conversation context preservation\",\n",
    "        \"âœ… Integrated with Azure AI services ecosystem\",\n",
    "        \"âœ… Automatic retry and error handling\"\n",
    "    ]\n",
    "    \n",
    "    for strength in foundry_strengths:\n",
    "        print(f\"   {strength}\")\n",
    "    \n",
    "    print(\"\\nğŸ¤” Azure AI Foundry Agents - Considerations:\")\n",
    "    foundry_considerations = [\n",
    "        \"âš ï¸ Slightly higher latency due to agent orchestration\",\n",
    "        \"âš ï¸ More complex setup and configuration\",\n",
    "        \"âš ï¸ Additional dependency on Azure AI Projects service\",\n",
    "        \"âš ï¸ May have usage limits different from direct API\"\n",
    "    ]\n",
    "    \n",
    "    for consideration in foundry_considerations:\n",
    "        print(f\"   {consideration}\")\n",
    "\n",
    "# Direct OpenAI analysis\n",
    "if use_direct_openai:\n",
    "    print(\"\\nâ˜ï¸ Direct Azure OpenAI - Strengths:\")\n",
    "    direct_strengths = [\n",
    "        \"âœ… Lower latency for simple queries\",\n",
    "        \"âœ… Direct control over all parameters\",\n",
    "        \"âœ… Simpler implementation and debugging\",\n",
    "        \"âœ… Maximum compatibility with OpenAI APIs\",\n",
    "        \"âœ… Lower complexity and fewer dependencies\",\n",
    "        \"âœ… More predictable performance characteristics\"\n",
    "    ]\n",
    "    \n",
    "    for strength in direct_strengths:\n",
    "        print(f\"   {strength}\")\n",
    "    \n",
    "    print(\"\\nğŸ¤” Direct Azure OpenAI - Considerations:\")\n",
    "    direct_considerations = [\n",
    "        \"âš ï¸ Manual conversation state management required\",\n",
    "        \"âš ï¸ Custom implementation needed for advanced features\",\n",
    "        \"âš ï¸ More code required for error handling and retries\",\n",
    "        \"âš ï¸ Tool integration requires custom development\"\n",
    "    ]\n",
    "    \n",
    "    for consideration in direct_considerations:\n",
    "        print(f\"   {consideration}\")\n",
    "\n",
    "# Hybrid architecture recommendations\n",
    "print(\"\\nğŸ¯ Hybrid Architecture Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "recommendations = {\n",
    "    \"Use Azure AI Foundry Agents for:\": [\n",
    "        \"ğŸ“ Complex multi-turn conversations\",\n",
    "        \"ğŸ”§ Applications requiring tool integration\",\n",
    "        \"ğŸ“Š Advanced workflow orchestration\",\n",
    "        \"ğŸ¯ Applications needing sophisticated agent behavior\",\n",
    "        \"ğŸ“ˆ Enterprise scenarios with complex requirements\"\n",
    "    ],\n",
    "    \"Use Direct Azure OpenAI for:\": [\n",
    "        \"âš¡ Simple, fast query-response scenarios\",\n",
    "        \"ğŸ”„ High-throughput applications\",\n",
    "        \"ğŸ›ï¸ Applications needing fine-grained control\",\n",
    "        \"ğŸ—ï¸ Custom integration scenarios\",\n",
    "        \"ğŸ’° Cost-sensitive implementations\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in recommendations.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for item in items:\n",
    "        print(f\"   {item}\")\n",
    "\n",
    "# Routing strategy for hybrid system\n",
    "print(\"\\nğŸ§­ Routing Strategy for Hybrid System:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "routing_strategy = {\n",
    "    \"Route to Local Models:\": [\n",
    "        \"ğŸ  Simple queries and quick responses\",\n",
    "        \"âš¡ Latency-sensitive applications\", \n",
    "        \"ğŸ”’ Privacy-sensitive data processing\",\n",
    "        \"ğŸ’° Cost optimization for high-volume queries\"\n",
    "    ],\n",
    "    \"Route to Azure AI Foundry Agents:\": [\n",
    "        \"ğŸ¤– Complex reasoning and analysis tasks\",\n",
    "        \"ğŸ’¬ Multi-turn conversations requiring context\",\n",
    "        \"ğŸ”§ Tasks requiring tool integration\",\n",
    "        \"ğŸ“Š Strategic planning and business analysis\"\n",
    "    ],\n",
    "    \"Route to Direct Azure OpenAI:\": [\n",
    "        \"âš¡ Fast complex queries without conversation state\",\n",
    "        \"ğŸ“ Document processing and summarization\",\n",
    "        \"ğŸ¨ Creative content generation\",\n",
    "        \"ğŸ”„ Batch processing scenarios\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in routing_strategy.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for item in items:\n",
    "        print(f\"   {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ Azure AI analysis complete - ready for Lab 4 routing implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89ca2c",
   "metadata": {},
   "source": [
    "## Step 3.12: Create Helper Functions for Future Labs\n",
    "\n",
    "Let's create reusable functions for both Azure AI Foundry Agents and direct OpenAI that we'll use in subsequent labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper functions for future labs\n",
    "\n",
    "class AzureAIManager:\n",
    "    \"\"\"Unified manager for Azure AI services (both Foundry Agents and direct OpenAI).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.project_client = project_client\n",
    "        self.azure_client = azure_client\n",
    "        self.foundry_agent = foundry_agent\n",
    "        self.agent_thread = agent_thread\n",
    "        self.use_foundry_agents = use_foundry_agents\n",
    "        self.use_direct_openai = use_direct_openai\n",
    "        \n",
    "    def query(self, prompt: str, method: str = \"auto\", **kwargs) -> tuple:\n",
    "        \"\"\"Query Azure AI with automatic or specified method selection.\"\"\"\n",
    "        if method == \"auto\":\n",
    "            return query_azure_intelligent(prompt, **kwargs)\n",
    "        elif method == \"foundry\" and self.use_foundry_agents:\n",
    "            return query_with_foundry_agent(prompt, **kwargs)\n",
    "        elif method == \"direct\" and self.use_direct_openai:\n",
    "            return query_with_direct_openai(prompt, **kwargs)\n",
    "        else:\n",
    "            return f\"Method '{method}' not available\", 0, False\n",
    "    \n",
    "    def get_capabilities(self) -> dict:\n",
    "        \"\"\"Get information about available Azure AI capabilities.\"\"\"\n",
    "        return {\n",
    "            \"foundry_agents_available\": self.use_foundry_agents,\n",
    "            \"direct_openai_available\": self.use_direct_openai,\n",
    "            \"primary_method\": \"foundry\" if self.use_foundry_agents else \"direct\" if self.use_direct_openai else \"none\",\n",
    "            \"deployment\": AZURE_OPENAI_DEPLOYMENT,\n",
    "            \"endpoint\": AZURE_OPENAI_ENDPOINT\n",
    "        }\n",
    "    \n",
    "    def create_conversation_thread(self):\n",
    "        \"\"\"Create a new conversation thread (Foundry Agents only).\"\"\"\n",
    "        if self.use_foundry_agents and self.project_client:\n",
    "            try:\n",
    "                return self.project_client.agents.threads.create()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to create thread: {e}\")\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "# Initialize the manager\n",
    "azure_ai_manager = AzureAIManager()\n",
    "\n",
    "# Test the manager\n",
    "capabilities = azure_ai_manager.get_capabilities()\n",
    "print(\"ğŸ”§ Azure AI Manager initialized\")\n",
    "print(\"ğŸ“Š Capabilities:\")\n",
    "for key, value in capabilities.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Quick test\n",
    "test_response, test_time, test_success = azure_ai_manager.query(\n",
    "    \"Briefly describe the benefits of using Azure AI Foundry for enterprise applications.\",\n",
    "    method=\"auto\"\n",
    ")\n",
    "\n",
    "if test_success:\n",
    "    print(f\"\\nâœ… Manager test successful ({test_time:.3f}s)\")\n",
    "    print(f\"ğŸ“ Response preview: {test_response[:150]}...\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Manager test failed: {test_response}\")\n",
    "\n",
    "print(\"\\nâœ… Helper functions created and tested for future labs\")\n",
    "print(\"ğŸ”„ Azure AI Manager ready for integration with hybrid routing system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db10983",
   "metadata": {},
   "source": [
    "## ğŸ‰ Lab 3 (Alternate) Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "- âœ… Successfully explored Azure AI Foundry Agents capabilities\n",
    "- âœ… Implemented both agent-based and direct OpenAI approaches\n",
    "- âœ… Tested complex document analysis and strategic planning tasks\n",
    "- âœ… Compared performance characteristics between different methods\n",
    "- âœ… Evaluated multi-turn conversation management\n",
    "- âœ… Created a unified Azure AI manager for future integration\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Azure AI Foundry Agents:**\n",
    "- **Strengths**: Advanced conversation management, built-in tool integration, sophisticated agent behavior\n",
    "- **Best for**: Complex multi-turn scenarios, enterprise applications requiring advanced capabilities\n",
    "- **Trade-offs**: Slightly higher latency, more complex setup\n",
    "\n",
    "**Direct Azure OpenAI:**\n",
    "- **Strengths**: Lower latency, simpler implementation, direct control\n",
    "- **Best for**: Fast responses, high-throughput scenarios, simple integrations\n",
    "- **Trade-offs**: Manual state management, custom feature development required\n",
    "\n",
    "### Hybrid Architecture Strategy:\n",
    "\n",
    "1. **Local Models**: Fast, private processing for simple queries\n",
    "2. **Azure AI Foundry Agents**: Complex reasoning, multi-turn conversations, advanced workflows\n",
    "3. **Direct Azure OpenAI**: Fast cloud processing, document analysis, creative tasks\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to Lab 4 to implement intelligent routing between local models and Azure AI services\n",
    "- The Azure AI Manager is ready for integration with the hybrid routing system\n",
    "- Performance characteristics will inform routing decisions\n",
    "\n",
    "### Advanced Capabilities Unlocked:\n",
    "âœ¨ **Conversation State Management**: Automatic context preservation across interactions\n",
    "\n",
    "ğŸ”§ **Tool Integration Ready**: Foundation for adding external tool capabilities\n",
    "\n",
    "ğŸ“Š **Enterprise-Grade**: Suitable for complex business applications\n",
    "\n",
    "ğŸ¯ **Flexible Routing**: Multiple Azure AI approaches for different use cases\n",
    "\n",
    "**Ready to build the intelligent three-way router in Lab 4!** ğŸš€\n",
    "\n",
    "*This alternate Lab 3 provides a more sophisticated foundation for enterprise hybrid AI systems, leveraging the full capabilities of Azure AI Foundry while maintaining compatibility with direct OpenAI approaches.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
