{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d6e023",
   "metadata": {},
   "source": [
    "# Lab 3: Cloud LLM Integration and Testing\n",
    "\n",
    "**Purpose:** Learn how to call the Azure cloud-based LLM for more complex tasks. This lab ensures that the Azure Foundry deployment is accessible and can handle demanding queries, albeit with higher latency than local. It highlights how to handle *heavy* tasks like summarization using the cloud model.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, we'll:\n",
    "- Connect to Azure OpenAI service through Azure AI Foundry\n",
    "- Test complex queries that require more sophisticated reasoning\n",
    "- Compare cloud vs local performance characteristics\n",
    "- Understand when to use cloud models in our hybrid architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd2b05",
   "metadata": {},
   "source": [
    "## Step 3.1: Load Configuration and Connect to Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e11199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load configuration from Lab 1\n",
    "load_dotenv()\n",
    "\n",
    "# Local model configuration\n",
    "LOCAL_ENDPOINT = os.environ[\"LOCAL_MODEL_ENDPOINT\"]\n",
    "LOCAL_MODEL_ALIAS = os.environ[\"LOCAL_MODEL_NAME\"]\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv('AZURE_DEPLOYMENT_NAME')\n",
    "AZURE_OPENAI_API_VERSION = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "\n",
    "print(f\"Azure endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"Azure deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"API version: {AZURE_OPENAI_API_VERSION}\")\n",
    "\n",
    "# Verify configuration\n",
    "if not all([AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, AZURE_OPENAI_DEPLOYMENT]):\n",
    "    print(\"‚ùå Missing Azure OpenAI configuration. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ Azure OpenAI configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc2269",
   "metadata": {},
   "source": [
    "## Step 3.2: Initialize Azure OpenAI Client\n",
    "\n",
    "Configure the Azure OpenAI client for accessing the cloud-based model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e529230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Azure OpenAI client initialized\")\n",
    "\n",
    "def query_azure_model(prompt, chat_history=None, max_tokens=500):\n",
    "    \"\"\"Send a query to the Azure model and return the response with timing.\"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Add current prompt to history\n",
    "    messages = chat_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = azure_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        return content, response_time, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0, False\n",
    "\n",
    "print(\"‚úÖ Azure query function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb09ad",
   "metadata": {},
   "source": [
    "## Step 3.3: Test Basic Azure Connectivity\n",
    "\n",
    "Let's start with a simple test to ensure our Azure connection is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic connectivity with a simple query\n",
    "test_prompt = \"Hello, Azure! Please respond with a brief greeting.\"\n",
    "\n",
    "print(\"Testing Azure OpenAI connectivity...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response, response_time, success = query_azure_model(test_prompt, max_tokens=50)\n",
    "\n",
    "if success:\n",
    "    print(f\"Query: {test_prompt}\")\n",
    "    print(f\"Azure response: {response}\")\n",
    "    print(f\"Response time: {response_time:.3f} seconds\")\n",
    "    print(\"‚úÖ Azure OpenAI connection successful!\")\n",
    "else:\n",
    "    print(f\"‚ùå Connection failed: {response}\")\n",
    "    print(\"Please check your Azure configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9898b",
   "metadata": {},
   "source": [
    "## Step 3.4: Test Complex Tasks - Document Summarization\n",
    "\n",
    "Now let's test the Azure model with a complex task that would challenge a local model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample long text for summarization (simulating a document)\n",
    "long_document = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming industries across the globe at an unprecedented pace. \n",
    "From healthcare to finance, from transportation to entertainment, AI technologies are reshaping \n",
    "how we work, live, and interact with the world around us.\n",
    "\n",
    "In healthcare, AI is enabling more accurate diagnostics through medical imaging analysis, \n",
    "drug discovery acceleration, and personalized treatment recommendations. Machine learning \n",
    "algorithms can now detect patterns in medical data that might be missed by human practitioners, \n",
    "leading to earlier disease detection and improved patient outcomes.\n",
    "\n",
    "The financial sector has embraced AI for fraud detection, algorithmic trading, and risk assessment. \n",
    "Banks use machine learning models to analyze transaction patterns in real-time, identifying \n",
    "suspicious activities and protecting customers from financial crimes. Robo-advisors powered by \n",
    "AI are democratizing investment management, making sophisticated financial strategies accessible \n",
    "to retail investors.\n",
    "\n",
    "Transportation is being revolutionized by autonomous vehicles and AI-powered traffic management \n",
    "systems. Self-driving cars promise to reduce accidents caused by human error, while intelligent \n",
    "traffic systems optimize routes and reduce congestion in urban areas.\n",
    "\n",
    "However, the rapid adoption of AI also raises important ethical and societal questions. \n",
    "Concerns about job displacement, privacy, algorithmic bias, and the concentration of AI \n",
    "capabilities in the hands of a few large corporations are driving discussions about AI governance \n",
    "and regulation.\n",
    "\n",
    "As we move forward, it's crucial to develop AI systems that are not only powerful and efficient \n",
    "but also transparent, fair, and aligned with human values. The future of AI depends on our \n",
    "ability to harness its potential while addressing its challenges responsibly.\n",
    "\"\"\"\n",
    "\n",
    "# Test document summarization\n",
    "summarization_prompt = f\"Please provide a concise summary of the following document:\\n\\n{long_document}\"\n",
    "\n",
    "print(\"Testing Azure model with document summarization:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Document length: {len(long_document)} characters\")\n",
    "print(f\"Document word count: ~{len(long_document.split())} words\")\n",
    "print(\"\\nRequesting summarization...\")\n",
    "\n",
    "summary, response_time, success = query_azure_model(summarization_prompt, max_tokens=300)\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nüìù Azure Summary:\")\n",
    "    print(summary)\n",
    "    print(f\"\\n‚è±Ô∏è  Response time: {response_time:.3f} seconds\")\n",
    "    print(f\"üìä Summary length: {len(summary)} characters\")\n",
    "    print(f\"üîÑ Compression ratio: {len(long_document)/len(summary):.1f}:1\")\n",
    "else:\n",
    "    print(f\"‚ùå Summarization failed: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4635d858",
   "metadata": {},
   "source": [
    "## Step 3.5: Test Creative and Analytical Tasks\n",
    "\n",
    "Let's test the Azure model with tasks that require creativity and deep analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d489361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creative writing\n",
    "creative_tasks = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"prompt\": \"Write a short poem about the intersection of technology and nature, focusing on harmony rather than conflict.\",\n",
    "        \"max_tokens\": 200\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Business Analysis\",\n",
    "        \"prompt\": \"Analyze the potential business impact of hybrid AI systems (combining local and cloud processing) for enterprise customers. Include benefits, challenges, and market opportunities.\",\n",
    "        \"max_tokens\": 400\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical Explanation\",\n",
    "        \"prompt\": \"Explain quantum computing and its implications for current encryption methods. Make it accessible to a business audience without deep technical background.\",\n",
    "        \"max_tokens\": 350\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing Azure model with complex creative and analytical tasks:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_time = 0\n",
    "successful_tasks = 0\n",
    "\n",
    "for i, task in enumerate(creative_tasks, 1):\n",
    "    print(f\"\\n{i}. {task['name']}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Prompt: {task['prompt']}\")\n",
    "    print(\"\\nGenerating response...\")\n",
    "    \n",
    "    response, response_time, success = query_azure_model(\n",
    "        task['prompt'], \n",
    "        max_tokens=task['max_tokens']\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        successful_tasks += 1\n",
    "        total_time += response_time\n",
    "        \n",
    "        print(f\"\\nü§ñ Azure Response:\")\n",
    "        print(response)\n",
    "        print(f\"\\n‚è±Ô∏è  Time: {response_time:.3f} seconds\")\n",
    "        print(f\"üìè Length: {len(response)} characters\")\n",
    "    else:\n",
    "        print(f\"‚ùå Task failed: {response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if successful_tasks > 0:\n",
    "    avg_response_time = total_time / successful_tasks\n",
    "    print(f\"\\nüìä Complex Task Performance Summary:\")\n",
    "    print(f\"Successful tasks: {successful_tasks}/{len(creative_tasks)}\")\n",
    "    print(f\"Average response time: {avg_response_time:.3f} seconds\")\n",
    "    print(f\"Total processing time: {total_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a42ef1",
   "metadata": {},
   "source": [
    "## Step 3.6: Compare Local vs Cloud Performance\n",
    "\n",
    "Let's directly compare the same queries on both local and cloud models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundry_local import FoundryLocalManager\n",
    "\n",
    "# Initialize and optionally bootstrap with a model\n",
    "manager = FoundryLocalManager(alias_or_model_id=None, bootstrap=True)\n",
    "\n",
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache: {local_models}\")\n",
    "\n",
    "print(f\"Local model alias: {local_models[0].alias}\")\n",
    "\n",
    "print(f\"Local model ID: {local_models[0].id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5de366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load local model functions from Lab 2\n",
    "from openai import OpenAI\n",
    "\n",
    "try:\n",
    "    # Recreate local client\n",
    "    local_client = OpenAI(\n",
    "        base_url=f\"{LOCAL_ENDPOINT}/v1\",\n",
    "        api_key=\"not-needed\"\n",
    "    )\n",
    "    \n",
    "    local_available = True\n",
    "    print(\"‚úÖ Local model configuration loaded\")\n",
    "except Exception as e:\n",
    "    local_available = False\n",
    "    print(f\"‚ö†Ô∏è  Local model not available: {e}\")\n",
    "    print(\"Comparison will focus on Azure model only\")\n",
    "\n",
    "def query_local_model_simple(prompt):\n",
    "    \"\"\"Simple local model query function.\"\"\"\n",
    "    if not local_available:\n",
    "        return \"Local model not available\", 0, False\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = local_client.chat.completions.create(\n",
    "            model=local_models[0].id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        return content, end_time - start_time, True\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9aa9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison queries\n",
    "comparison_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain machine learning in simple terms.\",\n",
    "    \"Write a brief summary of renewable energy benefits.\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ Comparing Local vs Azure Model Performance:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(comparison_queries, 1):\n",
    "    print(f\"\\nQuery {i}: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test local model\n",
    "    if local_available:\n",
    "        local_response, local_time, local_success = query_local_model_simple(query)\n",
    "        print(f\"üè† LOCAL ({local_time:.3f}s): {local_response[:100]}{'...' if len(local_response) > 100 else ''}\")\n",
    "    else:\n",
    "        print(f\"üè† LOCAL: Not available\")\n",
    "    \n",
    "    # Test Azure model\n",
    "    azure_response, azure_time, azure_success = query_azure_model(query, max_tokens=150)\n",
    "    if azure_success:\n",
    "        print(f\"‚òÅÔ∏è  AZURE ({azure_time:.3f}s): {azure_response[:100]}{'...' if len(azure_response) > 100 else ''}\")\n",
    "    else:\n",
    "        print(f\"‚òÅÔ∏è  AZURE: {azure_response}\")\n",
    "    \n",
    "    # Compare performance if both succeeded\n",
    "    if local_available and local_success and azure_success:\n",
    "        speedup = azure_time / local_time if local_time > 0 else float('inf')\n",
    "        print(f\"‚ö° Speed comparison: Local is {speedup:.1f}x faster than Azure\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b05c13",
   "metadata": {},
   "source": [
    "## Step 3.7: Error Handling and Rate Limiting\n",
    "\n",
    "Let's test Azure's error handling and understand potential limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases and error handling\n",
    "edge_cases = [\n",
    "    {\n",
    "        \"name\": \"Very Long Input\",\n",
    "        \"prompt\": \"Explain this: \" + \"AI is transforming the world. \" * 100,  # Repeat to test limits\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Empty Input\",\n",
    "        \"prompt\": \"\",\n",
    "        \"max_tokens\": 50\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Large Token Request\",\n",
    "        \"prompt\": \"Write a comprehensive essay about artificial intelligence.\",\n",
    "        \"max_tokens\": 2000  # Test if this exceeds limits\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing Azure model error handling and limits:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_case in edge_cases:\n",
    "    print(f\"\\nüß™ Testing: {test_case['name']}\")\n",
    "    print(f\"Prompt length: {len(test_case['prompt'])} characters\")\n",
    "    print(f\"Requested max tokens: {test_case['max_tokens']}\")\n",
    "    \n",
    "    response, response_time, success = query_azure_model(\n",
    "        test_case['prompt'], \n",
    "        max_tokens=test_case['max_tokens']\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"‚úÖ Success ({response_time:.3f}s)\")\n",
    "        print(f\"Response length: {len(response)} characters\")\n",
    "        if len(response) > 200:\n",
    "            print(f\"Response preview: {response[:200]}...\")\n",
    "        else:\n",
    "            print(f\"Full response: {response}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed: {response}\")\n",
    "    \n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e30390",
   "metadata": {},
   "source": [
    "## Step 3.8: Create Helper Functions for Future Labs\n",
    "\n",
    "Let's create reusable functions for the Azure model that we'll use in subsequent labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_azure_client():\n",
    "    \"\"\"Get configured Azure OpenAI client.\"\"\"\n",
    "    return AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "    )\n",
    "\n",
    "def query_azure_with_history(prompt, chat_history=None, max_tokens=500):\n",
    "    \"\"\"Query Azure model with optional chat history.\"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Add current prompt to history\n",
    "    messages = chat_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = azure_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        return content, response_time, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0, False\n",
    "\n",
    "print(\"‚úÖ Azure helper functions created and saved for future labs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34829f8e",
   "metadata": {},
   "source": [
    "## Step 3.9: Performance Summary and Analysis\n",
    "\n",
    "Let's summarize what we've learned about the Azure model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple performance summary\n",
    "print(\"üìä Azure Model Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "azure_strengths = [\n",
    "    \"‚úÖ Excellent at complex reasoning and analysis\",\n",
    "    \"‚úÖ High-quality creative content generation\",\n",
    "    \"‚úÖ Comprehensive document summarization\",\n",
    "    \"‚úÖ Detailed technical explanations\",\n",
    "    \"‚úÖ Large context window for long inputs\",\n",
    "    \"‚úÖ Latest knowledge and capabilities\"\n",
    "]\n",
    "\n",
    "azure_considerations = [\n",
    "    \"‚ö†Ô∏è  Higher latency due to network calls (typically 2-5 seconds)\",\n",
    "    \"‚ö†Ô∏è  Requires internet connectivity\",\n",
    "    \"‚ö†Ô∏è  Usage costs per token/request\",\n",
    "    \"‚ö†Ô∏è  Data privacy considerations (data sent to cloud)\",\n",
    "    \"‚ö†Ô∏è  Potential rate limiting during high usage\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Azure Model Strengths:\")\n",
    "for strength in azure_strengths:\n",
    "    print(f\"   {strength}\")\n",
    "\n",
    "print(\"\\nü§î Considerations:\")\n",
    "for consideration in azure_considerations:\n",
    "    print(f\"   {consideration}\")\n",
    "\n",
    "print(\"\\nüí° Optimal Use Cases for Azure Model:\")\n",
    "optimal_cases = [\n",
    "    \"üìù Document analysis and summarization\",\n",
    "    \"üé® Creative writing and content generation\",\n",
    "    \"üß† Complex reasoning and problem-solving\",\n",
    "    \"üìä Business analysis and strategic planning\",\n",
    "    \"üî¨ Technical explanations and tutorials\",\n",
    "    \"üåê Multi-language tasks and translations\"\n",
    "]\n",
    "\n",
    "for case in optimal_cases:\n",
    "    print(f\"   {case}\")\n",
    "\n",
    "print(\"\\nüîÑ This analysis will guide our routing logic in Lab 4!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48528990",
   "metadata": {},
   "source": [
    "## üéâ Lab 3 Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ‚úÖ Successfully connected to Azure OpenAI through Azure AI Foundry\n",
    "- ‚úÖ Tested complex tasks like document summarization and creative writing\n",
    "- ‚úÖ Compared performance characteristics with local models\n",
    "- ‚úÖ Understood error handling and service limitations\n",
    "- ‚úÖ Created reusable functions for the hybrid system\n",
    "\n",
    "### Key Findings:\n",
    "- **Quality**: Azure models excel at complex reasoning and high-quality generation\n",
    "- **Capability**: Can handle long documents, creative tasks, and detailed analysis\n",
    "- **Latency**: Higher response times (2-5 seconds) due to network and computation\n",
    "- **Cost**: Pay-per-use model with token-based pricing\n",
    "- **Reliability**: Robust error handling but subject to rate limits\n",
    "\n",
    "### Hybrid Architecture Insights:\n",
    "1. **Azure models excel at**: Complex analysis, creative tasks, summarization, detailed explanations\n",
    "2. **Trade-offs**: Higher quality and capability vs. increased latency and cost\n",
    "3. **Complementary to local**: Local provides speed, Azure provides sophistication\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to Lab 4 to implement intelligent routing between local and Azure models\n",
    "- The performance characteristics discovered here will inform our routing decisions\n",
    "- Both local and Azure helper functions are now ready for integration\n",
    "\n",
    "### Routing Strategy Preview:\n",
    "Based on our testing, we'll route queries to:\n",
    "- **Local**: Simple questions, greetings, basic calculations, quick responses\n",
    "- **Azure**: Summarization, analysis, creative writing, complex reasoning\n",
    "\n",
    "Ready to build the intelligent router in Lab 4! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
