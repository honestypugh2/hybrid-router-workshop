{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4492162",
   "metadata": {},
   "source": [
    "# Lab 2: On-Device LLM Quickstart and Testing\n",
    "\n",
    "**Purpose:** Use the Foundry Local model to handle a simple query in a notebook, demonstrating how to interact with the local LLM via code. This lab focuses on verifying **low-latency local inference** for basic tasks.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, we'll:\n",
    "- Connect to the local Foundry model via Python\n",
    "- Send simple queries to test functionality\n",
    "- Measure response times to demonstrate low latency\n",
    "- Understand the capabilities and limitations of local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13aadbe",
   "metadata": {},
   "source": [
    "## Step 2.1: Load Configuration and Initialize Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c41ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add parent directory for module imports\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from modules.config import Config, AzureFoundryConfig\n",
    "\n",
    "af = AzureFoundryConfig()\n",
    "af.azure_ai_foundry_endpoint\n",
    "\n",
    "# Config().get_azure_foundry_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00969d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "# Add parent directory for module imports\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "# from modules.config import config\n",
    "\n",
    "# # Test configuration access and show available configurations\n",
    "# print(\"Configuration loaded successfully!\")\n",
    "# print(f\"Debug mode: {config.is_debug_mode()}\")\n",
    "# print(f\"Local model endpoint: {config.get_local_model_endpoint()}\")\n",
    "# print(f\"Telemetry enabled: {config.is_telemetry_enabled()}\")\n",
    "\n",
    "# # Access specific configurations using the helper methods\n",
    "# local_endpoint = config.get_local_model_endpoint()\n",
    "# openai_key = config.get_azure_openai_key()\n",
    "# complexity_threshold = config.get_complexity_threshold()\n",
    "\n",
    "# print(f\"\\nConfiguration values:\")\n",
    "# print(f\"  Local endpoint: {local_endpoint}\")\n",
    "# print(f\"  OpenAI key configured: {'Yes' if openai_key else 'No'}\")\n",
    "# print(f\"  Complexity threshold: {complexity_threshold}\")\n",
    "\n",
    "# # Display environment info for debugging\n",
    "# env_info = config.get_environment_info()\n",
    "# print(f\"\\nEnvironment configuration status:\")\n",
    "# for section, details in env_info.items():\n",
    "#     print(f\"  {section}: {details}\")\n",
    "\n",
    "# Local model configuration\n",
    "LOCAL_ENDPOINT = os.environ[\"LOCAL_MODEL_ENDPOINT\"] \n",
    "LOCAL_MODEL_ALIAS = os.environ[\"LOCAL_MODEL_NAME\"]\n",
    "AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "print(f\"Local endpoint: {LOCAL_ENDPOINT}\")\n",
    "print(f\"Local model alias: {LOCAL_MODEL_ALIAS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8c4e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in cache: [FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-cpu, execution_provider=CPUExecutionProvider, device_type=CPU, file_size=2590 MB, license=MIT)]\n",
      "Model alias: phi-3.5-mini\n",
      "Local endpoint: http://127.0.0.1:57149/v1\n"
     ]
    }
   ],
   "source": [
    "from foundry_local import FoundryLocalManager\n",
    "\n",
    "# Initialize and optionally bootstrap with a model\n",
    "manager = FoundryLocalManager(alias_or_model_id=None, bootstrap=True)\n",
    "\n",
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache: {local_models}\")\n",
    "print(f\"Model alias: {local_models[0].alias}\")\n",
    "print(f\"Local endpoint: {manager.endpoint}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90333251",
   "metadata": {},
   "source": [
    "## Step 2.2: Connect to Foundry Local via Python\n",
    "\n",
    "We'll establish a connection to the local model service and configure the AzureOpenAI client to use the local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ef891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify local service is running\n",
    "def check_local_service():\n",
    "    try:\n",
    "        response = requests.get(f\"{LOCAL_ENDPOINT}/openai/status\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "if check_local_service():\n",
    "    print(\"âœ… Local model service is running\")\n",
    "else:\n",
    "    print(\"âŒ Local model service is not accessible\")\n",
    "    print(\"Please ensure 'foundry model run phi-3.5-mini' is running in a terminal\")\n",
    "    \n",
    "# Configure OpenAI client for local endpoint\n",
    "local_client = OpenAI(\n",
    "    base_url=f\"{LOCAL_ENDPOINT}/v1\",  # Foundry Local typically uses OpenAI-compatible API\n",
    "    api_key=\"not-needed\"  # Local service doesn't require authentication\n",
    ")\n",
    "\n",
    "print(\"âœ… Local OpenAI client configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3e893",
   "metadata": {},
   "source": [
    "## Step 2.3: Single-turn Query to Local Model\n",
    "\n",
    "Let's test the local model with a simple factual question that should be handled well by a lightweight model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming response\n",
    "try:\n",
    "    response = local_client.chat.completions.create(\n",
    "        model=local_models[0].id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Hello, what is the capital of France?\"}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(\"Streaming response:\")\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "    print()  # New line after streaming\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Streaming error: {e}\")\n",
    "    print(\"Streaming might not be supported by this local service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_local_model(prompt):\n",
    "    \"\"\"Send a query to the local model and return the response with timing.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Ensure we're using the properly configured client\n",
    "        response = local_client.chat.completions.create(\n",
    "            model=local_models[0].id,  # Use the alias directly\n",
    "            # base_url=manager.endpoint,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.7,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        return content, response_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0\n",
    "\n",
    "# Test with a simple factual question\n",
    "test_prompt = \"Hello, what is the capital of France?\"\n",
    "response, response_time = query_local_model(test_prompt)\n",
    "\n",
    "print(f\"Query: {test_prompt}\")\n",
    "print(f\"Local model response: {response}\")\n",
    "print(f\"Response time: {response_time:.3f} seconds\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2c354",
   "metadata": {},
   "source": [
    "## Step 2.4: Test Various Simple Queries\n",
    "\n",
    "Let's test the local model with different types of simple queries to understand its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770de56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries that should work well with a local model\n",
    "simple_queries = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Hi there! How are you today?\",\n",
    "    \"What is the largest planet in our solar system?\",\n",
    "    \"Convert 100 degrees Fahrenheit to Celsius.\",\n",
    "    \"What year was Python first released?\"\n",
    "]\n",
    "\n",
    "print(\"Testing local model with simple queries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_time = 0\n",
    "successful_queries = 0\n",
    "\n",
    "for i, query in enumerate(simple_queries, 1):\n",
    "    print(f\"\\nQuery {i}: {query}\")\n",
    "    response, response_time = query_local_model(query)\n",
    "    \n",
    "    if not response.startswith(\"Error:\"):\n",
    "        successful_queries += 1\n",
    "        total_time += response_time\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Time: {response_time:.3f} seconds\")\n",
    "    else:\n",
    "        print(f\"âŒ {response}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "if successful_queries > 0:\n",
    "    avg_response_time = total_time / successful_queries\n",
    "    print(f\"\\nðŸ“Š Summary:\")\n",
    "    print(f\"Successful queries: {successful_queries}/{len(simple_queries)}\")\n",
    "    print(f\"Average response time: {avg_response_time:.3f} seconds\")\n",
    "    print(f\"âœ… Local model demonstrates low-latency responses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5426f9",
   "metadata": {},
   "source": [
    "## Step 2.5: Test Local Model Limitations\n",
    "\n",
    "Let's try some more complex queries to understand where the local model might struggle, which will justify the need for cloud fallback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries that might challenge a local model\n",
    "complex_queries = [\n",
    "    \"Explain quantum computing in detail and discuss its implications for cryptography.\",\n",
    "    \"Write a comprehensive business plan for a sustainable energy startup.\",\n",
    "    \"Analyze the economic impact of artificial intelligence on employment over the next decade.\"\n",
    "]\n",
    "\n",
    "print(\"Testing local model with complex queries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(complex_queries, 1):\n",
    "    print(f\"\\nComplex Query {i}: {query}\")\n",
    "    response, response_time = query_local_model(query)\n",
    "    \n",
    "    if not response.startswith(\"Error:\"):\n",
    "        print(f\"Response: {response[:200]}...\" if len(response) > 200 else f\"Response: {response}\")\n",
    "        print(f\"Response length: {len(response)} characters\")\n",
    "        print(f\"Time: {response_time:.3f} seconds\")\n",
    "    else:\n",
    "        print(f\"âŒ {response}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nðŸ’¡ Observations:\")\n",
    "print(\"- Local models excel at simple, factual queries\")\n",
    "print(\"- Complex queries may receive shorter or less detailed responses\")\n",
    "print(\"- This demonstrates the need for intelligent routing to cloud models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abb31a",
   "metadata": {},
   "source": [
    "## Step 2.6: Demonstrate Offline Capability\n",
    "\n",
    "One key advantage of local models is that they work offline. Let's demonstrate this capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e67e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate offline capability by testing without internet dependency\n",
    "import platform\n",
    "import getpass\n",
    "\n",
    "# Device-specific information (works offline)\n",
    "device_info_prompt = f\"\"\"\n",
    "I'm running on a {platform.system()} system. \n",
    "The current user is {getpass.getuser()}.\n",
    "Can you help me with basic system information or simple calculations?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing offline/device-specific capability:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response, response_time = query_local_model(device_info_prompt)\n",
    "\n",
    "print(f\"Device-specific query: {device_info_prompt.strip()}\")\n",
    "print(f\"Local response: {response}\")\n",
    "print(f\"Response time: {response_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nðŸ”’ Privacy Benefits:\")\n",
    "print(\"- All processing happens locally\")\n",
    "print(\"- No data sent to external servers\")\n",
    "print(\"- Works without internet connection\")\n",
    "print(\"- Ideal for sensitive or personal queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29946951",
   "metadata": {},
   "source": [
    "## Step 2.7: Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of our local model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb61040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance with varying query lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "queries_by_length = [\n",
    "    (\"Hi\", \"Very short\"),\n",
    "    (\"What is the weather like today?\", \"Short\"),\n",
    "    (\"Can you explain what machine learning is and how it works in simple terms?\", \"Medium\"),\n",
    "    (\"Please provide a detailed explanation of how neural networks function, including the concepts of forward propagation, backpropagation, and gradient descent, along with practical applications in modern AI systems.\", \"Long\")\n",
    "]\n",
    "\n",
    "response_times = []\n",
    "query_lengths = []\n",
    "labels = []\n",
    "\n",
    "print(\"Analyzing performance vs query complexity:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query, label in queries_by_length:\n",
    "    response, response_time = query_local_model(query)\n",
    "    \n",
    "    if not response.startswith(\"Error:\"):\n",
    "        response_times.append(response_time)\n",
    "        query_lengths.append(len(query))\n",
    "        labels.append(label)\n",
    "        \n",
    "        print(f\"\\n{label} query ({len(query)} chars): {response_time:.3f}s\")\n",
    "        print(f\"Query: {query[:50]}{'...' if len(query) > 50 else ''}\")\n",
    "\n",
    "# Simple performance summary\n",
    "if response_times:\n",
    "    print(f\"\\nðŸ“ˆ Performance Summary:\")\n",
    "    print(f\"Fastest response: {min(response_times):.3f}s\")\n",
    "    print(f\"Slowest response: {max(response_times):.3f}s\")\n",
    "    print(f\"Average response time: {sum(response_times)/len(response_times):.3f}s\")\n",
    "    \n",
    "    # Show that local responses are consistently fast\n",
    "    if max(response_times) < 2.0:  # If all responses under 2 seconds\n",
    "        print(\"âœ… All local responses were under 2 seconds - excellent for user experience!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Some responses took longer - consider query complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0eeba",
   "metadata": {},
   "source": [
    "## Step 2.8: Create Helper Functions for Future Labs\n",
    "\n",
    "Let's create reusable functions that we'll use in subsequent labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df462c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_client():\n",
    "    \"\"\"Get configured OpenAI client for local model.\"\"\"\n",
    "    return OpenAI(\n",
    "        base_url=f\"{LOCAL_ENDPOINT}/v1\",\n",
    "        api_key=\"not-needed\"\n",
    "    )\n",
    "\n",
    "def query_local_with_history(prompt, chat_history=None):\n",
    "    \"\"\"Query local model with optional chat history.\"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Add current prompt to history\n",
    "    messages = chat_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = get_local_client().chat.completions.create(\n",
    "            model=LOCAL_MODEL_ALIAS,\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            temperature=0.7,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        return content, response_time, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0, False\n",
    "\n",
    "print(\"âœ… Helper functions created and saved for future labs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80605fd2",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Lab 2 Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "- âœ… Successfully connected to the local Foundry model via Python\n",
    "- âœ… Demonstrated low-latency responses for simple queries\n",
    "- âœ… Identified the strengths and limitations of local models\n",
    "- âœ… Measured performance characteristics\n",
    "- âœ… Created reusable functions for future labs\n",
    "\n",
    "### Key Findings:\n",
    "- **Speed**: Local models provide near-instant responses (typically < 1 second)\n",
    "- **Privacy**: All processing happens on-device with no external data transmission\n",
    "- **Availability**: Works offline without internet connectivity\n",
    "- **Limitations**: Less capable with complex reasoning or lengthy generation tasks\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to Lab 3 to set up and test the Azure cloud model\n",
    "- Keep your local model service running\n",
    "- The helper functions created here will be used for the hybrid routing system\n",
    "\n",
    "### Key Takeaways for Hybrid Architecture:\n",
    "1. **Local models excel at**: Simple Q&A, basic calculations, greetings, quick responses\n",
    "2. **Local models struggle with**: Complex reasoning, long-form content, specialized knowledge\n",
    "3. **This justifies hybrid routing**: Use local for speed, cloud for complexity\n",
    "\n",
    "The stage is now set to compare these local capabilities with cloud model performance in Lab 3!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
