{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c2c9ff",
   "metadata": {},
   "source": [
    "# Lab 1: Environment Setup (Foundry Local and Azure Foundry)\n",
    "\n",
    "**Purpose:** Configure both the local and cloud environments required for the hybrid architecture.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, we'll set up:\n",
    "- Azure Foundry Local for on-device LLM inference\n",
    "- Azure AI Foundry project for cloud-based LLM capabilities\n",
    "- Required Python libraries and environment variables\n",
    "\n",
    "This creates the foundation for building a hybrid AI chatbot that can route queries between local and cloud models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b641d",
   "metadata": {},
   "source": [
    "## Step 1.1: Install Azure Foundry Local\n",
    "\n",
    "Azure Foundry Local allows you to run LLMs directly on your device for fast, private inference.\n",
    "\n",
    "### Installation Instructions:\n",
    "\n",
    "**Windows:**\n",
    "```bash\n",
    "winget install Microsoft.FoundryLocal\n",
    "```\n",
    "\n",
    "**macOS:**\n",
    "```bash\n",
    "brew tap microsoft/foundrylocal && brew install foundrylocal\n",
    "```\n",
    "\n",
    "After installation, verify by running the following in a terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Foundry Local installation\n",
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run(['foundry', '--version'], capture_output=True, text=True)\n",
    "    print(f\"Foundry Local version: {result.stdout.strip()}\")\n",
    "    print(\"✅ Foundry Local is installed successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Foundry Local not found. Please install it using the instructions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07341950",
   "metadata": {},
   "source": [
    "## Step 1.2: Download and Start a Local Model\n",
    "\n",
    "We'll use a lightweight model like `phi-3.5-mini` that's optimized for local execution. \n",
    "\n",
    "*You can choose another lightweight model. For a list of models, please run `foundry model list` in the terminal before executing the command below.*\n",
    "\n",
    "**Run this command in your terminal (not in the notebook):**\n",
    "```bash\n",
    "foundry model run phi-3.5-mini\n",
    "```\n",
    "\n",
    "This will:\n",
    "1. Download the model (may take several minutes for first run)\n",
    "2. Start a local service hosting the model\n",
    "3. Provide an endpoint for API calls\n",
    "\n",
    "Keep this terminal open throughout the workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e66ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundry_local import FoundryLocalManager\n",
    "\n",
    "# Initialize and optionally bootstrap with a model\n",
    "manager = FoundryLocalManager(alias_or_model_id=None, bootstrap=True)\n",
    "\n",
    "print(\"Is service running: \", manager.is_service_running())\n",
    "print(\"Service URI: \", manager.service_uri)\n",
    "print(\"Service Endpoint: \", manager.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for module imports\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120771f",
   "metadata": {},
   "source": [
    "Let's update our `.env` file with the endpoint for Azure Foundry Local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv, set_key\n",
    "\n",
    "load_dotenv(find_dotenv(\".env\"))\n",
    "\n",
    "# Update a variable\n",
    "key_to_update = \"LOCAL_MODEL_ENDPOINT\"\n",
    "new_value = manager.service_uri\n",
    "\n",
    "# Update the .env file\n",
    "set_key(os.path.join(parent_dir, \".env\"), key_to_update, new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if local model service is running\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    # Common local endpoint for Foundry Local\n",
    "    local_endpoint = manager.service_uri # Adjust if your Foundry Local uses a different port\n",
    "    response = requests.get(f\"{local_endpoint}/openai/status\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Local model service is running\")\n",
    "        print(f\"Endpoint: {local_endpoint}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Local service responded with status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"❌ Local model service is not running or not accessible\")\n",
    "    print(\"Please run 'foundry model run phi-3.5-mini' in a terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71b198",
   "metadata": {},
   "source": [
    "## Step 1.3: Set Up Azure AI Foundry Project\n",
    "\n",
    "### Prerequisites:\n",
    "1. An Azure subscription with access to Azure OpenAI services\n",
    "2. An Azure AI Foundry project created\n",
    "3. A deployed chat model (e.g., GPT-4, GPT-3.5-turbo)\n",
    "\n",
    "### Required Information:\n",
    "If using Azure AI Foundry Agents\n",
    "- Azure AI Foundry endpoint URL\n",
    "- Credential\n",
    "\n",
    "If using Azure OpenAI direct\n",
    "- Azure OpenAI endpoint URL\n",
    "- API key\n",
    "- Version\n",
    "- Deployment name\n",
    "\n",
    "If you don't have these set up yet, follow the Azure AI Foundry documentation to create a project and deploy a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e38de",
   "metadata": {},
   "source": [
    "## Step 1.4: Configure Environment Variables\n",
    "\n",
    "Set up the configuration for both local and cloud connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf959f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(\".env\"))\n",
    "\n",
    "# Local model configuration\n",
    "LOCAL_ENDPOINT = os.environ[\"LOCAL_MODEL_ENDPOINT\"]  # Adjust if your Foundry Local uses a different port\n",
    "LOCAL_MODEL_ALIAS = os.environ[\"LOCAL_MODEL_NAME\"]\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_KEY = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "AZURE_OPENAI_DEPLOYMENT = os.environ[\"AZURE_DEPLOYMENT_NAME\"]\n",
    "AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "print(\"✅ Environment variables configured\")\n",
    "print(f\"Local endpoint: {LOCAL_ENDPOINT}\")\n",
    "print(f\"Azure endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"Azure deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"Azure API version: {AZURE_OPENAI_API_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37b109",
   "metadata": {},
   "source": [
    "## Step 1.5: Install Required Libraries\n",
    "\n",
    "Install the Python packages needed for the workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages - This is only needed if you haven't installed the packages yet\n",
    "# !pip install openai azure-ai-ml streamlit requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a719065",
   "metadata": {},
   "source": [
    "## Step 1.6: Test Connectivity\n",
    "\n",
    "Let's verify that we can connect to both local and cloud services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c39558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "\n",
    "# Test local connectivity\n",
    "print(\"Testing local model connectivity...\")\n",
    "try:\n",
    "    # For local Foundry service, we'll use a simple health check\n",
    "    response = requests.get(f\"{LOCAL_ENDPOINT}/openai/status\", timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Local model service is accessible\")\n",
    "    else:\n",
    "        print(f\"⚠️  Local service responded with status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Local model connection failed: {e}\")\n",
    "\n",
    "# Test Azure connectivity\n",
    "print(\"\\nTesting Azure OpenAI connectivity...\")\n",
    "try:\n",
    "    # Initialize Azure OpenAI client\n",
    "    azure_client = AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_OPENAI_DEPLOYMENT}\",\n",
    "        api_version=AZURE_OPENAI_API_VERSION\n",
    "    )\n",
    "    \n",
    "    # Test with a simple completion\n",
    "    response = azure_client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello, Azure!\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(\"✅ Azure OpenAI connection successful\")\n",
    "    print(f\"Test response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Azure OpenAI connection failed: {e}\")\n",
    "    print(\"Please check your endpoint, API key, and deployment name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754184ed",
   "metadata": {},
   "source": [
    "## Step 1.7: Save Configuration\n",
    "\n",
    "Create a configuration file for use in subsequent labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4347da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config file for the workshop\n",
    "config_content = f'''\n",
    "# Hybrid LLM Router Workshop Configuration\n",
    "\n",
    "# Local Model Configuration\n",
    "LOCAL_ENDPOINT={LOCAL_ENDPOINT}\n",
    "LOCAL_MODEL_ALIAS={LOCAL_MODEL_ALIAS}\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_ENDPOINT={AZURE_OPENAI_ENDPOINT}\n",
    "AZURE_OPENAI_KEY={AZURE_OPENAI_KEY}\n",
    "AZURE_OPENAI_DEPLOYMENT={AZURE_OPENAI_DEPLOYMENT}\n",
    "AZURE_OPENAI_API_VERSION={AZURE_OPENAI_API_VERSION}\n",
    "'''\n",
    "\n",
    "# with open('.env', 'w') as f:\n",
    "#     f.write(config_content)\n",
    "\n",
    "print(\"✅ Configuration saved to .env file\")\n",
    "print(\"This file will be used by subsequent labs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32daab0",
   "metadata": {},
   "source": [
    "## 🎉 Lab 1 Complete!\n",
    "\n",
    "You have successfully:\n",
    "- ✅ Installed Azure Foundry Local\n",
    "- ✅ Started a local model service\n",
    "- ✅ Configured Azure AI Foundry connection\n",
    "- ✅ Installed required Python libraries\n",
    "- ✅ Verified connectivity to both services\n",
    "- ✅ Saved configuration for future labs\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to Lab 2 to test the on-device LLM\n",
    "- Keep your local model service running throughout the workshop\n",
    "- If you encounter issues, check the troubleshooting section in the workshop documentation\n",
    "\n",
    "### Troubleshooting:\n",
    "- If the local model isn't starting, ensure you have enough RAM and disk space\n",
    "- If Azure connection fails, verify your subscription has access to Azure OpenAI\n",
    "- Check firewall settings if you can't connect to localhost:8080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
