{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9feb33a",
   "metadata": {},
   "source": [
    "# Lab 7: Frontend Chat Interface Integration (Streamlit)\n",
    "\n",
    "**Purpose:** Build a user-friendly web interface for the hybrid chatbot using Streamlit, allowing stakeholders to interact with the system and see the routing decisions in action.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, we'll:\n",
    "- Create a Streamlit web application for the chatbot\n",
    "- Integrate all previous lab components into a cohesive UI\n",
    "- Display routing transparency and conversation history\n",
    "- Add real-time performance monitoring\n",
    "- Test the complete hybrid system through the web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824f2ca",
   "metadata": {},
   "source": [
    "## Step 7.1: Import Dependencies and Load Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "# import pickle\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "from azure.ai.agents.models import CodeInterpreterTool\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Add parent directory to path for module imports\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5809686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foundry_local import FoundryLocalManager\n",
    "\n",
    "# Initialize and optionally bootstrap with a model\n",
    "manager = FoundryLocalManager(alias_or_model_id=None, bootstrap=True)\n",
    "\n",
    "LOCAL_ENDPOINT = manager.service_uri\n",
    "LOCAL_MODEL_ALIAS = os.environ[\"LOCAL_MODEL_NAME\"]\n",
    "AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "print(f\"Local service: {LOCAL_ENDPOINT}\")\n",
    "print(f\"Local endpoint: {manager.endpoint}\")\n",
    "print(f\"Local model alias: {LOCAL_MODEL_ALIAS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache: {local_models}\")\n",
    "print(f\"Model Alias: {local_models[0].alias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd0309",
   "metadata": {},
   "source": [
    "## Step 7.2: Create Core Routing Functions\n",
    "\n",
    "Let's recreate the essential functions from previous labs for the Streamlit app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_characteristics(query):\n",
    "    \"\"\"Analyze query characteristics for routing decisions.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    analysis = {\n",
    "        'length': len(query),\n",
    "        'word_count': len(query.split()),\n",
    "        'has_complex_keywords': False,\n",
    "        'is_greeting': False,\n",
    "        'is_simple_question': False,\n",
    "        'is_calculation': False\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower().strip()\n",
    "    \n",
    "    # Complex keywords\n",
    "    complex_keywords = [\n",
    "        'summarize', 'analyze', 'explain in detail', 'comprehensive',\n",
    "        'business plan', 'strategy', 'compare', 'evaluate', 'assess'\n",
    "    ]\n",
    "    \n",
    "    # Check patterns\n",
    "    greeting_patterns = [r\"^(hi|hello|hey|good morning)\"]\n",
    "    simple_patterns = [r\"^what is\", r\"^who is\", r\"^where is\"]\n",
    "    calc_patterns = [r'\\d+\\s*[+\\-*/]\\s*\\d+', r'calculate|compute']\n",
    "    \n",
    "    # Analyze\n",
    "    for keyword in complex_keywords:\n",
    "        if keyword in query_lower:\n",
    "            analysis['has_complex_keywords'] = True\n",
    "            break\n",
    "    \n",
    "    for pattern in greeting_patterns:\n",
    "        if re.match(pattern, query_lower):\n",
    "            analysis['is_greeting'] = True\n",
    "            break\n",
    "    \n",
    "    for pattern in simple_patterns:\n",
    "        if re.match(pattern, query_lower):\n",
    "            analysis['is_simple_question'] = True\n",
    "            break\n",
    "    \n",
    "    for pattern in calc_patterns:\n",
    "        if re.search(pattern, query_lower):\n",
    "            analysis['is_calculation'] = True\n",
    "            break\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def route_query(query, analysis=None):\n",
    "    \"\"\"Determine routing based on query analysis.\"\"\"\n",
    "    if analysis is None:\n",
    "        analysis = analyze_query_characteristics(query)\n",
    "    \n",
    "    # Route to LOCAL for simple tasks\n",
    "    if analysis['is_greeting']:\n",
    "        return 'local', 'Simple greeting - fast local response'\n",
    "    \n",
    "    if analysis['is_calculation']:\n",
    "        return 'local', 'Mathematical calculation - local processing'\n",
    "    \n",
    "    if analysis['is_simple_question'] and analysis['word_count'] <= 10:\n",
    "        return 'local', 'Simple factual question - local efficient'\n",
    "    \n",
    "    if analysis['word_count'] <= 5:\n",
    "        return 'local', 'Very short query - likely simple'\n",
    "    \n",
    "    # Route to CLOUD for complex tasks\n",
    "    if analysis['has_complex_keywords']:\n",
    "        return 'cloud', 'Complex analysis keywords - requires cloud'\n",
    "    \n",
    "    if analysis['word_count'] > 20:\n",
    "        return 'cloud', 'Long query - sophisticated processing needed'\n",
    "    \n",
    "    # Default routing\n",
    "    if analysis['word_count'] <= 15:\n",
    "        return 'local', 'Default local for moderate queries'\n",
    "    else:\n",
    "        return 'cloud', 'Default cloud for longer queries'\n",
    "\n",
    "print(\"‚úÖ Routing functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce42043",
   "metadata": {},
   "source": [
    "## Step 7.3: Initialize Model Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_clients():\n",
    "    \"\"\"Initialize both local and Azure OpenAI clients.\"\"\"\n",
    "    clients = {'local': None, 'azure': None}\n",
    "    \n",
    "    # Local client setup\n",
    "    try:\n",
    "        LOCAL_ENDPOINT = os.environ.get(\"LOCAL_MODEL_ENDPOINT\", \"http://localhost:59413\")\n",
    "        # Use the correct model name from the local server\n",
    "        LOCAL_MODEL = \"Phi-3.5-mini-instruct-generic-cpu\"  # Fixed: actual model name from server\n",
    "        \n",
    "        clients['local'] = OpenAI(\n",
    "            base_url=f\"{LOCAL_ENDPOINT}/v1\",\n",
    "            api_key=\"not-needed\"\n",
    "        )\n",
    "        clients['local_model'] = LOCAL_MODEL\n",
    "        print(f\"‚úÖ Local client initialized: {LOCAL_MODEL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Local client failed: {e}\")\n",
    "    \n",
    "    # Azure client setup\n",
    "    try:\n",
    "        AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "        AZURE_OPENAI_KEY = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "        AZURE_OPENAI_DEPLOYMENT = os.environ[\"AZURE_DEPLOYMENT_NAME\"]\n",
    "        AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "        if all([AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, AZURE_OPENAI_DEPLOYMENT, AZURE_OPENAI_API_VERSION]):\n",
    "            # Initialize Azure OpenAI client with correct parameters\n",
    "            clients['azure'] = AzureOpenAI(\n",
    "                api_key=AZURE_OPENAI_KEY,\n",
    "                api_version=AZURE_OPENAI_API_VERSION,\n",
    "                azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "            )\n",
    "            clients['azure_model'] = AZURE_OPENAI_DEPLOYMENT\n",
    "            print(f\"‚úÖ Azure client initialized: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "        else:\n",
    "            print(\"‚ùå Azure configuration incomplete\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Azure client failed: {e}\")\n",
    "    \n",
    "    return clients\n",
    "\n",
    "# Initialize clients with fixed configuration\n",
    "clients = initialize_clients()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358a6f4",
   "metadata": {},
   "source": [
    "## Step 7.4: Create the Main Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_and_respond(user_message, chat_history=None):\n",
    "    \"\"\"Route message and generate response with telemetry.\"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Analyze and route\n",
    "    analysis = analyze_query_characteristics(user_message)\n",
    "    target, reason = route_query(user_message, analysis)\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = chat_history + [{\"role\": \"user\", \"content\": user_message}]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if target == \"local\" and clients['local']:\n",
    "            response = clients['local'].chat.completions.create(\n",
    "                model=clients['local_model'],\n",
    "                messages=messages,\n",
    "                max_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            source_tag = \"Local\"\n",
    "            \n",
    "        elif target == \"cloud\" and clients['azure']:\n",
    "            response = clients['azure'].chat.completions.create(\n",
    "                model=clients['azure_model'],\n",
    "                messages=messages,\n",
    "                max_tokens=400,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            source_tag = \"Cloud\"\n",
    "            \n",
    "        else:\n",
    "            # Fallback logic\n",
    "            if clients['azure']:\n",
    "                response = clients['azure'].chat.completions.create(\n",
    "                    model=clients['azure_model'],\n",
    "                    messages=messages,\n",
    "                    max_tokens=400,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                source_tag = \"Cloud (Fallback)\"\n",
    "            elif clients['local']:\n",
    "                response = clients['local'].chat.completions.create(\n",
    "                    model=clients['local_model'],\n",
    "                    messages=messages,\n",
    "                    max_tokens=200,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                source_tag = \"Local (Fallback)\"\n",
    "            else:\n",
    "                return \"Error: No models available\", 0, \"Error\", reason\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        return content, response_time, source_tag, reason\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0, \"Error\", reason\n",
    "\n",
    "print(\"‚úÖ Chat function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa359c",
   "metadata": {},
   "source": [
    "## Step 7.5: Create the Streamlit Application Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc885c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Streamlit app code\n",
    "streamlit_app_code = '''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import re\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Hybrid AI Chatbot\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"routing_stats\" not in st.session_state:\n",
    "    st.session_state.routing_stats = {\"local\": 0, \"cloud\": 0}\n",
    "\n",
    "def analyze_query_characteristics(query):\n",
    "    \"\"\"Analyze query characteristics for routing decisions.\"\"\"\n",
    "    analysis = {\n",
    "        \\'length\\': len(query),\n",
    "        \\'word_count\\': len(query.split()),\n",
    "        \\'has_complex_keywords\\': False,\n",
    "        \\'is_greeting\\': False,\n",
    "        \\'is_simple_question\\': False,\n",
    "        \\'is_calculation\\': False\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower().strip()\n",
    "    \n",
    "    # Complex keywords\n",
    "    complex_keywords = [\n",
    "        \\'summarize\\', \\'analyze\\', \\'explain in detail\\', \\'comprehensive\\',\n",
    "        \\'business plan\\', \\'strategy\\', \\'compare\\', \\'evaluate\\', \\'assess\\'\n",
    "    ]\n",
    "    \n",
    "    # Check patterns\n",
    "    greeting_patterns = [r\\'^\\'(hi|hello|hey|good morning)\\']\n",
    "    simple_patterns = [r\\'^\\'what is\\', r\\'^\\'who is\\', r\\'^\\'where is\\']\n",
    "    calc_patterns = [r\\'\\\\d+\\\\s*[+\\\\-*/]\\\\s*\\\\d+\\', r\\'calculate|compute\\']\n",
    "    \n",
    "    # Analyze\n",
    "    for keyword in complex_keywords:\n",
    "        if keyword in query_lower:\n",
    "            analysis[\\'has_complex_keywords\\'] = True\n",
    "            break\n",
    "    \n",
    "    for pattern in greeting_patterns:\n",
    "        if re.match(pattern, query_lower):\n",
    "            analysis[\\'is_greeting\\'] = True\n",
    "            break\n",
    "    \n",
    "    for pattern in simple_patterns:\n",
    "        if re.match(pattern, query_lower):\n",
    "            analysis[\\'is_simple_question\\'] = True\n",
    "            break\n",
    "    \n",
    "    for pattern in calc_patterns:\n",
    "        if re.search(pattern, query_lower):\n",
    "            analysis[\\'is_calculation\\'] = True\n",
    "            break\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def route_query(query, analysis=None):\n",
    "    \"\"\"Determine routing based on query analysis.\"\"\"\n",
    "    if analysis is None:\n",
    "        analysis = analyze_query_characteristics(query)\n",
    "    \n",
    "    # Route to LOCAL for simple tasks\n",
    "    if analysis[\\'is_greeting\\']:\n",
    "        return \\'local\\', \\'Simple greeting - fast local response\\'\n",
    "    \n",
    "    if analysis[\\'is_calculation\\']:\n",
    "        return \\'local\\', \\'Mathematical calculation - local processing\\'\n",
    "    \n",
    "    if analysis[\\'is_simple_question\\'] and analysis[\\'word_count\\'] <= 10:\n",
    "        return \\'local\\', \\'Simple factual question - local efficient\\'\n",
    "    \n",
    "    if analysis[\\'word_count\\'] <= 5:\n",
    "        return \\'local\\', \\'Very short query - likely simple\\'\n",
    "    \n",
    "    # Route to CLOUD for complex tasks\n",
    "    if analysis[\\'has_complex_keywords\\']:\n",
    "        return \\'cloud\\', \\'Complex analysis keywords - requires cloud\\'\n",
    "    \n",
    "    if analysis[\\'word_count\\'] > 20:\n",
    "        return \\'cloud\\', \\'Long query - sophisticated processing needed\\'\n",
    "    \n",
    "    # Default routing\n",
    "    if analysis[\\'word_count\\'] <= 15:\n",
    "        return \\'local\\', \\'Default local for moderate queries\\'\n",
    "    else:\n",
    "        return \\'cloud\\', \\'Default cloud for longer queries\\'\n",
    "\n",
    "@st.cache_resource\n",
    "def initialize_clients():\n",
    "    \"\"\"Initialize both local and Azure OpenAI clients.\"\"\"\n",
    "    clients = {\\'local\\': None, \\'azure\\': None}\n",
    "    \n",
    "    # Local client setup\n",
    "    try:\n",
    "        LOCAL_ENDPOINT = os.environ.get(\"LOCAL_MODEL_ENDPOINT\", \"http://localhost:52329\")\n",
    "        LOCAL_MODEL = os.environ.get(\"LOCAL_MODEL_NAME\", \"phi-3.5-mini\")\n",
    "        \n",
    "        clients[\\'local\\'] = OpenAI(\n",
    "            base_url=f\"{LOCAL_ENDPOINT}/v1\",\n",
    "            api_key=\"not-needed\"\n",
    "        )\n",
    "        clients[\\'local_model\\'] = LOCAL_MODEL\n",
    "    except Exception as e:\n",
    "        st.error(f\"Local client failed: {e}\")\n",
    "    \n",
    "    # Azure client setup\n",
    "    try:\n",
    "        AZURE_ENDPOINT = os.getenv(\\'AZURE_OPENAI_ENDPOINT\\')\n",
    "        AZURE_KEY = os.getenv(\\'AZURE_OPENAI_KEY\\')\n",
    "        AZURE_DEPLOYMENT = os.getenv(\\'AZURE_DEPLOYMENT_NAME\\')\n",
    "        AZURE_VERSION = os.getenv(\\'AZURE_OPENAI_API_VERSION\\')\n",
    "        \n",
    "        if all([AZURE_ENDPOINT, AZURE_KEY, AZURE_DEPLOYMENT, AZURE_VERSION]):\n",
    "            clients[\\'azure\\'] = AzureOpenAI(\n",
    "                api_key=AZURE_KEY,\n",
    "                api_version=AZURE_VERSION,\n",
    "                azure_endpoint=AZURE_ENDPOINT\n",
    "            )\n",
    "            clients[\\'azure_model\\'] = AZURE_DEPLOYMENT\n",
    "    except Exception as e:\n",
    "        st.error(f\"Azure client failed: {e}\")\n",
    "    \n",
    "    return clients\n",
    "\n",
    "def route_and_respond(user_message, chat_history=None):\n",
    "    \"\"\"Route message and generate response with telemetry.\"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Analyze and route\n",
    "    analysis = analyze_query_characteristics(user_message)\n",
    "    target, reason = route_query(user_message, analysis)\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = chat_history + [{\\'role\\': \\'user\\', \\'content\\': user_message}]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if target == \"local\" and clients[\\'local\\']:\n",
    "            response = clients[\\'local\\'].chat.completions.create(\n",
    "                model=clients[\\'local_model\\'],\n",
    "                messages=messages,\n",
    "                max_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            source_tag = \"Local\"\n",
    "            \n",
    "        elif target == \"cloud\" and clients[\\'azure\\']:\n",
    "            response = clients[\\'azure\\'].chat.completions.create(\n",
    "                model=clients[\\'azure_model\\'],\n",
    "                messages=messages,\n",
    "                max_tokens=400,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            source_tag = \"Cloud\"\n",
    "            \n",
    "        else:\n",
    "            # Fallback logic\n",
    "            if clients[\\'azure\\']:\n",
    "                response = clients[\\'azure\\'].chat.completions.create(\n",
    "                    model=clients[\\'azure_model\\'],\n",
    "                    messages=messages,\n",
    "                    max_tokens=400,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                source_tag = \"Cloud (Fallback)\"\n",
    "            elif clients[\\'local\\']:\n",
    "                response = clients[\\'local\\'].chat.completions.create(\n",
    "                    model=clients[\\'local_model\\'],\n",
    "                    messages=messages,\n",
    "                    max_tokens=200,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                source_tag = \"Local (Fallback)\"\n",
    "            else:\n",
    "                return \"Error: No models available\", 0, \"Error\", reason\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        return content, response_time, source_tag, reason\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", 0, \"Error\", reason\n",
    "\n",
    "# Initialize clients\n",
    "clients = initialize_clients()\n",
    "\n",
    "# App header\n",
    "st.title(\"ü§ñ Hybrid AI Chatbot\")\n",
    "st.markdown(\"**Intelligent routing between local and cloud models**\")\n",
    "\n",
    "# Sidebar with stats and info\n",
    "with st.sidebar:\n",
    "    st.header(\"üìä System Status\")\n",
    "    \n",
    "    # Model availability\n",
    "    st.subheader(\"Model Availability\")\n",
    "    if clients[\\'local\\']:\n",
    "        st.success(\"üü¢ Local Model: Ready\")\n",
    "    else:\n",
    "        st.error(\"üî¥ Local Model: Unavailable\")\n",
    "    \n",
    "    if clients[\\'azure\\']:\n",
    "        st.success(\"üü¢ Cloud Model: Ready\")\n",
    "    else:\n",
    "        st.error(\"üî¥ Cloud Model: Unavailable\")\n",
    "    \n",
    "    # Routing statistics\n",
    "    st.subheader(\"Routing Statistics\")\n",
    "    total_queries = st.session_state.routing_stats[\\'local\\'] + st.session_state.routing_stats[\\'cloud\\']\n",
    "    if total_queries > 0:\n",
    "        local_pct = (st.session_state.routing_stats[\\'local\\'] / total_queries) * 100\n",
    "        cloud_pct = (st.session_state.routing_stats[\\'cloud\\'] / total_queries) * 100\n",
    "        st.metric(\"Local Queries\", st.session_state.routing_stats[\\'local\\'], f\"{local_pct:.1f}%\")\n",
    "        st.metric(\"Cloud Queries\", st.session_state.routing_stats[\\'cloud\\'], f\"{cloud_pct:.1f}%\")\n",
    "    else:\n",
    "        st.info(\"No queries processed yet\")\n",
    "    \n",
    "    # Clear conversation\n",
    "    if st.button(\"üóëÔ∏è Clear Conversation\"):\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.routing_stats = {\\'local\\': 0, \\'cloud\\': 0}\n",
    "        st.rerun()\n",
    "\n",
    "# Main chat interface\n",
    "st.subheader(\"üí¨ Chat Interface\")\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\\'role\\']):\n",
    "        if message[\\'role\\'] == \\'assistant\\':\n",
    "            # Show response with source and timing\n",
    "            st.markdown(message[\\'content\\'])\n",
    "            if \\'source\\' in message and \\'time\\' in message:\n",
    "                st.caption(f\"Source: {message[\\'source\\']} | Response time: {message[\\'time\\']:.3f}s\")\n",
    "        else:\n",
    "            st.markdown(message[\\'content\\'])\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask me anything...\"):\n",
    "    # Add user message to chat\n",
    "    st.session_state.messages.append({\\'role\\': \\'user\\', \\'content\\': prompt})\n",
    "    \n",
    "    # Display user message\n",
    "    with st.chat_message(\\'user\\'):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Generate and display assistant response\n",
    "    with st.chat_message(\\'assistant\\'):\n",
    "        with st.spinner(\\'Thinking...\\'):\n",
    "            # Convert messages to format expected by route_and_respond\n",
    "            chat_history = [{\\'role\\': msg[\\'role\\'], \\'content\\': msg[\\'content\\']} \n",
    "                          for msg in st.session_state.messages[:-1] \n",
    "                          if msg[\\'role\\'] != \\'assistant\\' or \\'content\\' in msg]\n",
    "            \n",
    "            response, response_time, source, reason = route_and_respond(prompt, chat_history)\n",
    "            \n",
    "            # Display response\n",
    "            st.markdown(response)\n",
    "            st.caption(f\"Source: {source} | Response time: {response_time:.3f}s\")\n",
    "            \n",
    "            # Update routing statistics\n",
    "            if source.lower().startswith(\\'local\\'):\n",
    "                st.session_state.routing_stats[\\'local\\'] += 1\n",
    "            elif source.lower().startswith(\\'cloud\\'):\n",
    "                st.session_state.routing_stats[\\'cloud\\'] += 1\n",
    "            \n",
    "            # Add assistant response to chat\n",
    "            st.session_state.messages.append({\n",
    "                \\'role\\': \\'assistant\\', \n",
    "                \\'content\\': response,\n",
    "                \\'source\\': source,\n",
    "                \\'time\\': response_time,\n",
    "                \\'reason\\': reason\n",
    "            })\n",
    "\n",
    "# Example queries\n",
    "st.subheader(\"üí° Try These Examples\")\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.markdown(\"**Simple Queries (Local)**\")\n",
    "    example_simple = [\n",
    "        \"Hi there!\",\n",
    "        \"What is 15 + 27?\",\n",
    "        \"What is the capital of France?\"\n",
    "    ]\n",
    "    for example in example_simple:\n",
    "        if st.button(f\"üí¨ {example}\", key=f\"simple_{example}\"):\n",
    "            # Trigger the example by setting it as the next input\n",
    "            st.session_state[\\'next_input\\'] = example\n",
    "\n",
    "with col2:\n",
    "    st.markdown(\"**Complex Queries (Cloud)**\")\n",
    "    example_complex = [\n",
    "        \"Analyze the benefits of hybrid AI\",\n",
    "        \"Write a summary of machine learning\",\n",
    "        \"Compare local vs cloud computing\"\n",
    "    ]\n",
    "    for example in example_complex:\n",
    "        if st.button(f\"‚òÅÔ∏è {example}\", key=f\"complex_{example}\"):\n",
    "            # Trigger the example by setting it as the next input\n",
    "            st.session_state[\\'next_input\\'] = example\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"**Hybrid AI Chatbot** - Demonstrating intelligent routing between local and cloud models\")\n",
    "'''\n",
    "\n",
    "# # Write the Streamlit app to file\n",
    "app_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'app.py')\n",
    "# with open(app_path, 'w', encoding='utf-8') as f:\n",
    "#     f.write(streamlit_app_code)\n",
    "\n",
    "print(f\"‚úÖ Streamlit app created: {app_path}\")\n",
    "print(\"To run the app, use: streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca157a",
   "metadata": {},
   "source": [
    "## Step 7.6: Test the Application Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that all components work together\n",
    "test_queries = [\n",
    "    \"Hello!\",\n",
    "    \"What is 25 * 4?\",\n",
    "    \"Analyze the benefits of hybrid AI systems\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing the chat system:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    \n",
    "    response, response_time, source, reason = route_and_respond(query)\n",
    "    \n",
    "    print(f\"Assistant ({source}): {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
    "    print(f\"‚è±Ô∏è Time: {response_time:.3f}s | Reason: {reason}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n‚úÖ Chat system test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13ee0d",
   "metadata": {},
   "source": [
    "## Step 7.7: Launch Instructions and Usage Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Lab 7 Complete! Here's how to use your Hybrid AI Chatbot:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìÇ Files Created:\")\n",
    "print(f\"   - app.py (Streamlit application)\")\n",
    "print(f\"   - lab7_frontend_chat_interface.ipynb (this notebook)\")\n",
    "\n",
    "print(\"\\nüîß To Launch the Web Interface:\")\n",
    "print(\"   1. Open a terminal in the workshop directory\")\n",
    "print(\"   2. Run: streamlit run app.py\")\n",
    "print(\"   3. The app will open in your browser automatically\")\n",
    "\n",
    "print(\"\\nüí¨ Using the Chat Interface:\")\n",
    "print(\"   ‚Ä¢ Type simple questions (greetings, math) ‚Üí Fast local responses\")\n",
    "print(\"   ‚Ä¢ Ask complex queries (analysis, summaries) ‚Üí Cloud processing\")\n",
    "print(\"   ‚Ä¢ Watch the sidebar for routing statistics\")\n",
    "print(\"   ‚Ä¢ See response times and source transparency\")\n",
    "\n",
    "print(\"\\nüéØ Demo Scenarios for Stakeholders:\")\n",
    "print(\"   1. Start with: 'Hello!' ‚Üí Shows instant local response\")\n",
    "print(\"   2. Ask: 'What is 15 + 27?' ‚Üí Math handled locally\")\n",
    "print(\"   3. Try: 'Analyze the benefits of hybrid AI' ‚Üí Complex cloud processing\")\n",
    "print(\"   4. Follow up: 'Summarize that in one sentence' ‚Üí Context maintained\")\n",
    "\n",
    "print(\"\\nüìä Features Demonstrated:\")\n",
    "print(\"   ‚úÖ Intelligent routing between local and cloud\")\n",
    "print(\"   ‚úÖ Transparent source indication\")\n",
    "print(\"   ‚úÖ Performance monitoring and statistics\")\n",
    "print(\"   ‚úÖ Conversation continuity across models\")\n",
    "print(\"   ‚úÖ Real-time response time tracking\")\n",
    "print(\"   ‚úÖ Fallback mechanisms for reliability\")\n",
    "\n",
    "print(\"\\nüéâ Your hybrid AI chatbot is ready for demonstration!\")\n",
    "print(\"   Use this to show stakeholders how local + cloud AI can work together seamlessly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eab954",
   "metadata": {},
   "source": [
    "## üéâ Lab 7 Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "- ‚úÖ Created a professional Streamlit web interface for the hybrid chatbot\n",
    "- ‚úÖ Integrated all previous lab components into a cohesive user experience\n",
    "- ‚úÖ Added real-time routing statistics and performance monitoring\n",
    "- ‚úÖ Implemented transparent source indication for user trust\n",
    "- ‚úÖ Built example scenarios for stakeholder demonstrations\n",
    "- ‚úÖ Created a production-ready frontend for the hybrid AI system\n",
    "\n",
    "### Key Features of the Web Interface:\n",
    "1. **Clean Chat Interface**: Familiar chat UI with message history\n",
    "2. **Routing Transparency**: Clear indication of local vs cloud responses\n",
    "3. **Performance Metrics**: Real-time response times and statistics\n",
    "4. **System Status**: Live monitoring of model availability\n",
    "5. **Example Queries**: Guided demonstrations for different complexity levels\n",
    "6. **Conversation Management**: Clear history and restart functionality\n",
    "\n",
    "### Demonstration Value:\n",
    "**For Stakeholders**: The web interface provides a tangible way to experience the hybrid AI system's benefits:\n",
    "- **Speed**: Instant responses for simple queries (local)\n",
    "- **Quality**: Sophisticated answers for complex tasks (cloud)\n",
    "- **Transparency**: Clear source indication builds trust\n",
    "- **Reliability**: Fallback mechanisms ensure system availability\n",
    "- **Intelligence**: Smart routing optimizes user experience\n",
    "\n",
    "### Workshop Conclusion:\n",
    "You now have a complete **hybrid LLM chatbot system** that demonstrates:\n",
    "- Intelligent routing between on-device and cloud models\n",
    "- Seamless user experience with transparent processing\n",
    "- Performance optimization for both speed and quality\n",
    "- Production-ready architecture with monitoring and fallbacks\n",
    "\n",
    "**Next Steps**: Use this system to gather feedback, refine routing logic, and explore additional capabilities like device-specific functions or enhanced local model fine-tuning.\n",
    "\n",
    "The hybrid AI future is here ‚Äì fast, smart, and transparent! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01592336",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting: Fixed Streamlit App\n",
    "\n",
    "If you're getting a 400 error, the issue is likely with the Azure OpenAI configuration. I've created a fixed version with better error handling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the fixed Streamlit app to replace the original\n",
    "import shutil\n",
    "\n",
    "# Copy the fixed app\n",
    "try:\n",
    "    shutil.copy('app_fixed.py', 'app.py')\n",
    "    print(\"‚úÖ Fixed Streamlit app copied to app.py\")\n",
    "    print(\"\\nüîß Key fixes applied:\")\n",
    "    print(\"   - Better error handling for Azure client\")\n",
    "    print(\"   - Debug information in sidebar\")\n",
    "    print(\"   - Improved environment variable checking\")\n",
    "    print(\"   - More detailed error messages\")\n",
    "    print(\"\\nüöÄ Try running: streamlit run app.py\")\n",
    "    print(\"   Check the sidebar for debug information if errors occur\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to copy fixed app: {e}\")\n",
    "    print(\"   You can manually run: streamlit run app_fixed.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de247c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check environment variables and local endpoint\n",
    "import os\n",
    "\n",
    "print(\"üîç Debugging local model configuration:\")\n",
    "print(f\"LOCAL_MODEL_ENDPOINT: {os.environ.get('LOCAL_MODEL_ENDPOINT', 'Not set')}\")\n",
    "print(f\"LOCAL_MODEL_NAME: {os.environ.get('LOCAL_MODEL_NAME', 'Not set')}\")\n",
    "\n",
    "# Test local endpoint directly\n",
    "try:\n",
    "    import requests\n",
    "    local_endpoint = os.environ.get(\"LOCAL_MODEL_ENDPOINT\", \"http://localhost:59413\")\n",
    "    print(f\"\\nüß™ Testing local endpoint: {local_endpoint}\")\n",
    "    \n",
    "    # Test health endpoint\n",
    "    health_response = requests.get(f\"{local_endpoint}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "    # Test models endpoint\n",
    "    models_response = requests.get(f\"{local_endpoint}/v1/models\", timeout=5)\n",
    "    print(f\"Models endpoint status: {models_response.status_code}\")\n",
    "    if models_response.status_code == 200:\n",
    "        models_data = models_response.json()\n",
    "        print(f\"Available models: {[model['id'] for model in models_data.get('data', [])]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Local endpoint test failed: {e}\")\n",
    "    print(\"   This explains the 400 error - local model server may not be running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the local model name mismatch\n",
    "print(\"\\nüîß Fixing local model configuration...\")\n",
    "\n",
    "# Update the clients with the correct model name\n",
    "def fix_initialize_clients():\n",
    "    \"\"\"Initialize both local and Azure OpenAI clients with correct model names.\"\"\"\n",
    "    clients = {'local': None, 'azure': None}\n",
    "    \n",
    "    # Local client setup with correct model name\n",
    "    try:\n",
    "        LOCAL_ENDPOINT = os.environ.get(\"LOCAL_MODEL_ENDPOINT\", \"http://localhost:59413\")\n",
    "        # Use the actual model name from the server\n",
    "        LOCAL_MODEL = \"Phi-3.5-mini-instruct-generic-cpu\"  # Fixed model name\n",
    "        \n",
    "        clients['local'] = OpenAI(\n",
    "            base_url=f\"{LOCAL_ENDPOINT}/v1\",\n",
    "            api_key=\"not-needed\"\n",
    "        )\n",
    "        clients['local_model'] = LOCAL_MODEL\n",
    "        print(f\"‚úÖ Local client fixed with model: {LOCAL_MODEL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Local client failed: {e}\")\n",
    "    \n",
    "    # Azure client setup (should already be working)\n",
    "    try:\n",
    "        AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "        AZURE_OPENAI_KEY = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "        AZURE_OPENAI_DEPLOYMENT = os.environ[\"AZURE_DEPLOYMENT_NAME\"]\n",
    "        AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "        if all([AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, AZURE_OPENAI_DEPLOYMENT, AZURE_OPENAI_API_VERSION]):\n",
    "            clients['azure'] = AzureOpenAI(\n",
    "                api_key=AZURE_OPENAI_KEY,\n",
    "                api_version=AZURE_OPENAI_API_VERSION,\n",
    "                azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "            )\n",
    "            clients['azure_model'] = AZURE_OPENAI_DEPLOYMENT\n",
    "            print(f\"‚úÖ Azure client confirmed: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "        else:\n",
    "            print(\"‚ùå Azure configuration incomplete\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Azure client failed: {e}\")\n",
    "    \n",
    "    return clients\n",
    "\n",
    "# Update the global clients variable\n",
    "clients = fix_initialize_clients()\n",
    "print(f\"\\nüìä Updated clients: {clients.keys()}\")\n",
    "print(f\"Local model: {clients.get('local_model', 'Not available')}\")\n",
    "print(f\"Azure model: {clients.get('azure_model', 'Not available')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed chat system\n",
    "test_queries_fixed = [\n",
    "    \"Hello!\",\n",
    "    \"What is 25 * 4?\", \n",
    "    \"Analyze the benefits of hybrid AI systems\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing FIXED chat system:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries_fixed:\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    \n",
    "    response, response_time, source, reason = route_and_respond(query)\n",
    "    \n",
    "    print(f\"Assistant ({source}): {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
    "    print(f\"‚è±Ô∏è Time: {response_time:.3f}s | Reason: {reason}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n‚úÖ Fixed chat system test completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
